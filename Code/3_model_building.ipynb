{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Building\n",
    "\n",
    "Using the labeled feature data set constructed in the `Code/2_feature_engineering.ipynb` Jupyter notebook, this notebook loads the feature data from Azure Blob container and splits it into a training and test data set. We then build two machine learning models, a decision tree classifier and a random forest classifier, to predict when different components within our machine population will fail. The two models are compared and we store the better performing model for deployment in an Azure web service. We will prepare and build the web service in the `Code/4_operationalization.ipynb` Jupyter notebook.\n",
    "\n",
    "**Note:** This notebook will take about 3-5 minutes to execute all cells, depending on the compute configuration you have setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# for creating pipelines and model\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# For some data handling\n",
    "import pandas as pd\n",
    "\n",
    "# For Azure blob storage access\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load feature data set\n",
    "\n",
    "We have previously created the labeled feature data set in the `Code\\2_feature_engineering.ipynb` Jupyter notebook. Since the Azure Blob storage account name and account key are not passed between notebooks, you'll need your credentials here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Azure blob storage details here \n",
    "ACCOUNT_NAME = \"<your blob storage account name>\"\n",
    "\n",
    "# You can find the account key under the _Access Keys_ link in the \n",
    "# [Azure Portal](portal.azure.com) page for your Azure storage container.\n",
    "ACCOUNT_KEY = \"<your blob storage account key>\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# The data from the feature engineering note book is stored in the feature engineering container.\n",
    "CONTAINER_NAME = CONTAINER_NAME = \"featureengineering\"\n",
    "\n",
    "# Connect to your blob service     \n",
    "az_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# We will store and read each of these data sets in blob storage in an \n",
    "# Azure Storage Container on your Azure subscription.\n",
    "# See https://github.com/Azure/ViennaDocs/blob/master/Documentation/UsingBlobForStorage.md\n",
    "# for details.\n",
    "\n",
    "# This is the final feature data file.\n",
    "FEATURES_LOCAL_DIRECT = 'featureengineering_files.parquet'\n",
    "\n",
    "# This is where we store the final model data file.\n",
    "LOCAL_DIRECT = 'model_result.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and dump a short summary of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE creating a local directory!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machineID</th>\n",
       "      <th>dt_truncated</th>\n",
       "      <th>volt_rollingmean_3</th>\n",
       "      <th>rotate_rollingmean_3</th>\n",
       "      <th>pressure_rollingmean_3</th>\n",
       "      <th>vibration_rollingmean_3</th>\n",
       "      <th>volt_rollingmean_24</th>\n",
       "      <th>rotate_rollingmean_24</th>\n",
       "      <th>pressure_rollingmean_24</th>\n",
       "      <th>vibration_rollingmean_24</th>\n",
       "      <th>...</th>\n",
       "      <th>error5sum_rollingmean_24</th>\n",
       "      <th>comp1sum</th>\n",
       "      <th>comp2sum</th>\n",
       "      <th>comp3sum</th>\n",
       "      <th>comp4sum</th>\n",
       "      <th>model</th>\n",
       "      <th>age</th>\n",
       "      <th>model_encoded</th>\n",
       "      <th>failure</th>\n",
       "      <th>label_e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>2016-01-01 06:00:00</td>\n",
       "      <td>147.813753</td>\n",
       "      <td>410.546469</td>\n",
       "      <td>103.110374</td>\n",
       "      <td>39.881874</td>\n",
       "      <td>166.464991</td>\n",
       "      <td>449.921760</td>\n",
       "      <td>100.608971</td>\n",
       "      <td>40.313560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>2016-01-01 03:00:00</td>\n",
       "      <td>161.893907</td>\n",
       "      <td>457.866635</td>\n",
       "      <td>106.671660</td>\n",
       "      <td>42.281086</td>\n",
       "      <td>167.917852</td>\n",
       "      <td>459.850110</td>\n",
       "      <td>99.954524</td>\n",
       "      <td>40.198525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>159.216094</td>\n",
       "      <td>466.617543</td>\n",
       "      <td>102.928240</td>\n",
       "      <td>39.135677</td>\n",
       "      <td>169.175332</td>\n",
       "      <td>456.416658</td>\n",
       "      <td>99.402692</td>\n",
       "      <td>39.688645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-31 21:00:00</td>\n",
       "      <td>173.141342</td>\n",
       "      <td>466.089834</td>\n",
       "      <td>102.410363</td>\n",
       "      <td>40.737921</td>\n",
       "      <td>170.269608</td>\n",
       "      <td>453.365861</td>\n",
       "      <td>97.793726</td>\n",
       "      <td>39.614332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-31 18:00:00</td>\n",
       "      <td>173.328305</td>\n",
       "      <td>445.790528</td>\n",
       "      <td>96.623228</td>\n",
       "      <td>39.309750</td>\n",
       "      <td>168.427467</td>\n",
       "      <td>452.489297</td>\n",
       "      <td>96.946852</td>\n",
       "      <td>39.826918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-31 15:00:00</td>\n",
       "      <td>177.571978</td>\n",
       "      <td>413.721281</td>\n",
       "      <td>101.407082</td>\n",
       "      <td>43.319996</td>\n",
       "      <td>166.300368</td>\n",
       "      <td>453.315787</td>\n",
       "      <td>97.547494</td>\n",
       "      <td>39.738175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-31 12:00:00</td>\n",
       "      <td>167.647979</td>\n",
       "      <td>445.761921</td>\n",
       "      <td>92.272371</td>\n",
       "      <td>39.157228</td>\n",
       "      <td>166.667170</td>\n",
       "      <td>453.450990</td>\n",
       "      <td>98.124989</td>\n",
       "      <td>39.282556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-31 09:00:00</td>\n",
       "      <td>165.986147</td>\n",
       "      <td>479.098619</td>\n",
       "      <td>99.443408</td>\n",
       "      <td>39.040782</td>\n",
       "      <td>164.224087</td>\n",
       "      <td>456.703897</td>\n",
       "      <td>99.921575</td>\n",
       "      <td>39.200765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-31 06:00:00</td>\n",
       "      <td>164.557066</td>\n",
       "      <td>503.854515</td>\n",
       "      <td>97.879841</td>\n",
       "      <td>38.605762</td>\n",
       "      <td>163.387522</td>\n",
       "      <td>454.477489</td>\n",
       "      <td>100.385824</td>\n",
       "      <td>39.571223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-31 03:00:00</td>\n",
       "      <td>171.953747</td>\n",
       "      <td>430.399022</td>\n",
       "      <td>102.257001</td>\n",
       "      <td>38.202043</td>\n",
       "      <td>163.563755</td>\n",
       "      <td>447.991413</td>\n",
       "      <td>101.242371</td>\n",
       "      <td>39.515151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   machineID        dt_truncated  volt_rollingmean_3  rotate_rollingmean_3  \\\n",
       "0         27 2016-01-01 06:00:00          147.813753            410.546469   \n",
       "1         27 2016-01-01 03:00:00          161.893907            457.866635   \n",
       "2         27 2016-01-01 00:00:00          159.216094            466.617543   \n",
       "3         27 2015-12-31 21:00:00          173.141342            466.089834   \n",
       "4         27 2015-12-31 18:00:00          173.328305            445.790528   \n",
       "5         27 2015-12-31 15:00:00          177.571978            413.721281   \n",
       "6         27 2015-12-31 12:00:00          167.647979            445.761921   \n",
       "7         27 2015-12-31 09:00:00          165.986147            479.098619   \n",
       "8         27 2015-12-31 06:00:00          164.557066            503.854515   \n",
       "9         27 2015-12-31 03:00:00          171.953747            430.399022   \n",
       "\n",
       "   pressure_rollingmean_3  vibration_rollingmean_3  volt_rollingmean_24  \\\n",
       "0              103.110374                39.881874           166.464991   \n",
       "1              106.671660                42.281086           167.917852   \n",
       "2              102.928240                39.135677           169.175332   \n",
       "3              102.410363                40.737921           170.269608   \n",
       "4               96.623228                39.309750           168.427467   \n",
       "5              101.407082                43.319996           166.300368   \n",
       "6               92.272371                39.157228           166.667170   \n",
       "7               99.443408                39.040782           164.224087   \n",
       "8               97.879841                38.605762           163.387522   \n",
       "9              102.257001                38.202043           163.563755   \n",
       "\n",
       "   rotate_rollingmean_24  pressure_rollingmean_24  vibration_rollingmean_24  \\\n",
       "0             449.921760               100.608971                 40.313560   \n",
       "1             459.850110                99.954524                 40.198525   \n",
       "2             456.416658                99.402692                 39.688645   \n",
       "3             453.365861                97.793726                 39.614332   \n",
       "4             452.489297                96.946852                 39.826918   \n",
       "5             453.315787                97.547494                 39.738175   \n",
       "6             453.450990                98.124989                 39.282556   \n",
       "7             456.703897                99.921575                 39.200765   \n",
       "8             454.477489               100.385824                 39.571223   \n",
       "9             447.991413               101.242371                 39.515151   \n",
       "\n",
       "    ...     error5sum_rollingmean_24  comp1sum  comp2sum  comp3sum  comp4sum  \\\n",
       "0   ...                          0.0     504.0     564.0     444.0     399.0   \n",
       "1   ...                          0.0     504.0     564.0     444.0     399.0   \n",
       "2   ...                          0.0     504.0     564.0     444.0     399.0   \n",
       "3   ...                          0.0     503.0     563.0     443.0     398.0   \n",
       "4   ...                          0.0     503.0     563.0     443.0     398.0   \n",
       "5   ...                          0.0     503.0     563.0     443.0     398.0   \n",
       "6   ...                          0.0     503.0     563.0     443.0     398.0   \n",
       "7   ...                          0.0     503.0     563.0     443.0     398.0   \n",
       "8   ...                          0.0     503.0     563.0     443.0     398.0   \n",
       "9   ...                          0.0     503.0     563.0     443.0     398.0   \n",
       "\n",
       "    model  age    model_encoded  failure  label_e  \n",
       "0  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "1  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "2  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "3  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "4  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "5  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "6  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "7  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "8  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "9  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the previous created final dataset into the workspace\n",
    "# create a local path where we store results\n",
    "if not os.path.exists(FEATURES_LOCAL_DIRECT):\n",
    "    os.makedirs(FEATURES_LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if FEATURES_LOCAL_DIRECT in blob.name:\n",
    "        local_file = os.path.join(FEATURES_LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        az_blob_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "feat_data = spark.read.parquet(FEATURES_LOCAL_DIRECT)\n",
    "\n",
    "feat_data.limit(10).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Training/Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with data that comes with time-stamps such as telemetry and errors as in this example, splitting of data into training, validation and test sets should be performed carefully to prevent overestimating the performance of the models. In predictive maintenance, the features are usually generated using laging aggregates and consecutive examples that fall into the same time window may have similar feature values in that window. If a random splitting of training and testing is used, it is possible for some portion of these similar examples that are in the same window to be selected for training and the other portion to leak into the testing data. Also, it is possible for training examples to be ahead of time than validation and testing examples when data is randomly split. However, predictive models should be trained on historical data and valiadted and tested on future data. Due to these problems, validation and testing based on random sampling may provide overly optimistic results. Since random sampling is not a viable approach here, cross validation methods that rely on random samples such as k-fold cross validation is not useful either.\n",
    "\n",
    "For predictive maintenance problems, a time-dependent spliting strategy is often a better approach to estimate performance which is done by validating and testing on examples that are later in time than the training examples. For a time-dependent split, a point in time is picked and model is trained on examples up to that point in time, and validated on the examples after that point assuming that the future data after the splitting point is not known. However, this effects the labelling of features falling into the labelling window right before the split as it is assumed that failure information is not known beyond the splitting cut-off. Due to that, those feature records can not be labeled and will not be used. This also prevents the leaking problem at the splitting point.\n",
    "\n",
    "Validation can be performed by picking different split points and examining the performance of the models trained on different time splits. In the following, we use a splitting points to train the model and look at the performances for the other split in the evaluation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of input columns for downstream modeling - note model variable was removed as string was not supported\n",
    "input_features = [\n",
    "'volt_rollingmean_3',\n",
    "'rotate_rollingmean_3',\n",
    "'pressure_rollingmean_3',\n",
    "'vibration_rollingmean_3',\n",
    "'volt_rollingmean_24',\n",
    "'rotate_rollingmean_24',\n",
    "'pressure_rollingmean_24',\n",
    "'vibration_rollingmean_24',\n",
    "'volt_rollingstd_3',\n",
    "'rotate_rollingstd_3',\n",
    "'pressure_rollingstd_3',\n",
    "'vibration_rollingstd_3',\n",
    "'volt_rollingstd_24',\n",
    "'rotate_rollingstd_24',\n",
    "'pressure_rollingstd_24',\n",
    "'vibration_rollingstd_24',\n",
    "'error1sum_rollingmean_24',\n",
    "'error2sum_rollingmean_24',\n",
    "'error3sum_rollingmean_24',\n",
    "'error4sum_rollingmean_24',\n",
    "'error5sum_rollingmean_24',\n",
    "'comp1sum',\n",
    "'comp2sum',\n",
    "'comp3sum',\n",
    "'comp4sum',\n",
    "'age' #,\n",
    "#'model_encoded'    \n",
    "]\n",
    "\n",
    "label_var = ['label_e']\n",
    "key_cols =['machineID','dt_truncated']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark models require a vectorized data frame. We transform the dataset here and then split the data into a training and test set. We use this split data to train the model on 9 months of data (training data), and evaluate on the remaining 3 months (test data) going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2174000\n",
      "747000\n"
     ]
    }
   ],
   "source": [
    "# assemble features\n",
    "va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "feat_data = va.transform(feat_data).select('machineID','dt_truncated','label_e','features')\n",
    "\n",
    "# set maxCategories so features with > 10 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \n",
    "                               outputCol=\"indexedFeatures\", \n",
    "                               maxCategories=10).fit(feat_data)\n",
    "\n",
    "# fit on whole dataset to include all labels in index\n",
    "labelIndexer = StringIndexer(inputCol=\"label_e\", outputCol=\"indexedLabel\").fit(feat_data)\n",
    "\n",
    "# split the data into train/test based on date\n",
    "training = feat_data.filter(feat_data.dt_truncated > \"2015-01-01\").filter(feat_data.dt_truncated < \"2015-09-30\")\n",
    "testing = feat_data.filter(feat_data.dt_truncated > \"2015-09-30\")\n",
    "\n",
    "print(training.count())\n",
    "print(testing.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models\n",
    "\n",
    "In predictive maintenance, machine failures are usually rare occurrences in the lifetime of the assets compared to normal operation. This causes an imbalance in the label distribution which usually causes poor performance as algorithms tend to classify majority class examples better at the expense of minority class examples as the total misclassification error is much improved when majority class is labeled correctly. This causes low recall rates although accuracy can be high and becomes a larger problem when the cost of false alarms to the business is very high. To help with this problem, sampling techniques such as oversampling of the minority examples are usually used along with more sophisticated techniques which are not covered in this notebook.\n",
    "\n",
    "Also, due to the class imbalance problem, it is important to look at evaluation metrics other than accuracy alone and compare those metrics to the baseline metrics which are computed when random chance is used to make predictions rather than a machine learning model. The comparison will bring out the value and benefits of using a machine learning model better.\n",
    "\n",
    "We will build and compare two models, a Random Forest Classifier and Decision Tree Classifier. To compare these models, we compute weighted precision/recall, F1 score along with the accuracy metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "Decision trees and their ensembles are popular methods for the machine learning tasks of classification and regression. Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions.\n",
    "\n",
    "Remember, we build the model by training on the training data set, then evaluate the model using the testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a DT model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# chain indexers and forest in a Pipeline\n",
    "pipeline_dt = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# train model.  This also runs the indexers.\n",
    "model_dt = pipeline_dt.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate this model, we predict the component failures over the test data set. The standard method of viewing this evaluation is with a _confusion matrix_ shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------+----+----+----+----+\n",
      "|indexedLabel_prediction|   0.0| 1.0| 2.0| 3.0| 4.0|\n",
      "+-----------------------+------+----+----+----+----+\n",
      "|                    0.0|734418|   4| 116|   1| 263|\n",
      "|                    1.0|     4|4698|   0|   0|   0|\n",
      "|                    2.0|     0|   0|3630|   0|   0|\n",
      "|                    3.0|   275|   0|   0|1868|   0|\n",
      "|                    4.0|    11|   0|   0|  28|1684|\n",
      "+-----------------------+------+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions.\n",
    "predictions_dt = model_dt.transform(testing)\n",
    "\n",
    "predictions_dt.stat.crosstab('indexedLabel', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix lists each true component failure in rows. Numer 0.0 corresponds to not failed. Then, numbers 1.0-4.0 correspond to each of the 4 components in the machine. Each column represents the predicted value. \n",
    "\n",
    "So, the second number in the top row indicates how many days we predicted component 1 would fail, when no components actually did fail. The second number in the second row, indicates how many days we correctly predicted component 1 would fail.\n",
    "\n",
    "We read the confusion matrix number along the diagonal as correctly classifying the component failure. Numbers above the diagonal indicate the model incorrectly predicting a failure when non occured, and those below indicate incorrectly predicting a non-failure for the indicated component failure.\n",
    "\n",
    "When evaluating classification models, it is convenient to reduce the results in the confusion matrix into a single performance statistic. However, depending on the problem space, it is impossible to always use the same statistic in this evaluation. Below, we calculate 4 such statistics.\n",
    "\n",
    "- **Accuracy**: reports how often we correctly predicted the labeled data. Unfortunatly, when there is a class imbalance (a large number of one of the labels relative to others), this measure is biased towards the largest class. In this case non-failure days.\n",
    "\n",
    "Because of the class imbalance inherint in predictive maintenance problems, it is better to look at the remaining statistics instead. Here positive predictions indicate a failure.\n",
    "\n",
    "- **Weighted Precision**: Precision is a measure of how well the model classifies the truely positive samples. Precision depends on falsely classifying negative days as positive.\n",
    "\n",
    "- **Weighted Recall**: Recall is a measure of how well the model can find the positive samples. Recall depends on falsely classifying positive days as negative.\n",
    "\n",
    "- **F1**: F1 considers both the precision and the recall. F1 score is the harmonic average of precision and recall. An F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
    "\n",
    "Below we calculate these evaluation statistics for the decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.99906\n",
      "Weighted Precision = 0.9991\n",
      "Weighted Recall = 0.99906\n",
      "F1 = 0.999061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\")\n",
    "print(\"Accuracy = %g\" % evaluator.evaluate(predictions_dt, {evaluator.metricName: \"accuracy\"}))\n",
    "print(\"Weighted Precision = %g\" % evaluator.evaluate(predictions_dt, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "print(\"Weighted Recall = %g\" % evaluator.evaluate(predictions_dt, {evaluator.metricName: \"weightedRecall\"}))\n",
    "print(\"F1 = %g\" % evaluator.evaluate(predictions_dt, {evaluator.metricName: \"f1\"}))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that this is a simulated data set. We would expect a model built on real world data to behave very differently. The accuracy may still be close to one, but the precision and recall numbers would be much lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "A random forest is an ensemble of decision trees. Random forests combine many decision trees in order to reduce the risk of overfitting. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=100)\n",
    "\n",
    "# chain indexers and forest in a Pipeline\n",
    "pipeline_rf = Pipeline(stages=[labelIndexer, featureIndexer, rf])\n",
    "\n",
    "# train model.  This also runs the indexers.\n",
    "model_rf = pipeline_rf.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again predict on the test data and show the confusion matrix as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------+----+----+----+---+\n",
      "|indexedLabel_prediction|   0.0| 1.0| 2.0| 3.0|4.0|\n",
      "+-----------------------+------+----+----+----+---+\n",
      "|                    0.0|734747|   3|  52|   0|  0|\n",
      "|                    1.0|     0|4702|   0|   0|  0|\n",
      "|                    2.0|   576|   1|3053|   0|  0|\n",
      "|                    3.0|    54|   0|   0|2089|  0|\n",
      "|                    4.0|  1664|   0|   0|   0| 59|\n",
      "+-----------------------+------+----+----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions.\n",
    "predictions_rf = model_rf.transform(testing)\n",
    "\n",
    "predictions_rf.stat.crosstab('indexedLabel', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calculate the performance statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.996854\n",
      "Weighted Precision = 0.996852\n",
      "Weighted Recall = 0.996854\n",
      "F1 = 0.995783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\")\n",
    "print(\"Accuracy = %g\" % evaluator.evaluate(predictions_rf, {evaluator.metricName: \"accuracy\"}))\n",
    "print(\"Weighted Precision = %g\" % evaluator.evaluate(predictions_rf, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "print(\"Weighted Recall = %g\" % evaluator.evaluate(predictions_rf, {evaluator.metricName: \"weightedRecall\"}))\n",
    "print(\"F1 = %g\" % evaluator.evaluate(predictions_rf, {evaluator.metricName: \"f1\"}))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these staticistics to those for the decision tree classifier above, we see that the random forest predicts (marginally) better. We store the random forest model in a serialized `Spark` model file for use in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_rf.write().overwrite().save(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']+'pdmrfull.model')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In the next notebook `Code\\4_operationalization.ipynb` Jupyter notebook we will create the functions needed to operationalize and deploy any model to get realtime predictions. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdmScenarioTest dsvmDocker",
   "language": "python",
   "name": "pdmscenariotest_dsvmdocker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

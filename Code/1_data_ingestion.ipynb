{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data source\n",
    "\n",
    "The common data elements for predictive maintenance problems can be summarized as follows:\n",
    "\n",
    "* Machine features: The features of a machine, e.g. engine size, make and model, location.\n",
    "* Operator features: The features of the operator, e.g. gender, past experience.\n",
    "* Machine conditions and usage: The operating conditions of a machine e.g. data collected from sensors.\n",
    "* Maintenance history: The repair history of a machine, e.g. error codes, previous maintenance activities or component replacements.\n",
    "* Failure history: The failure history of a machine or component within the machine.\n",
    "\n",
    "It is possible and usually the case that failure history is contained in maintenance history such as in the form of special error codes or order dates for spare parts. In those cases, failures can be extracted from the maintenance data. Additionally, different business domains may have a variety of other data sources that influence failure patterns which are not listed here exhaustively. These should be identified by consulting the domain experts when building predictive models.\n",
    "\n",
    "Some examples of above data elements from use cases are:\n",
    "    \n",
    "**Machine conditions and usage:** Flight routes and times, sensor data collected from aircraft engines, sensor readings from ATM transactions, train events data, sensor readings from wind turbines, elevators and connected cars.\n",
    "    \n",
    "**Machine features:** Circuit breaker technical specifications such as voltage levels, geolocation or car features such as make, model, engine size, tire types, production facility etc.\n",
    "\n",
    "**Failure history:** fight delay dates, aircraft component failure dates and types, ATM cash withdrawal transaction failures, train/elevator door failures, brake disk replacement order dates, wind turbine failure dates and circuit breaker command failures.\n",
    "\n",
    "**Maintenance history:** Flight error logs, ATM transaction error logs, train maintenance records including maintenance type, short description etc. and circuit breaker maintenance records.\n",
    "\n",
    "Given the above data sources, the two main data types we observe in predictive maintenance domain are temporal data and static data. Failure history, machine conditions, repair history, usage history almost always come with time-stamps indicating the time of collection for each piece of data. Machine features and operator features in general are static since they usually describe the technical specifications of machines or operatorâ€™s properties. It is possible for these features to change over time and if so they should be treated as time stamped data sources.\n",
    "\n",
    "# Step 1: Data Aquisition\n",
    "\n",
    "The data aquisiton notebook will download the simulated predicitive maintenance data sets from our GitHub store. Do some preliminary data cleaning and verification, and store the results in an Azure Blob storage container for use in the remaining steps of this analysis.\n",
    "\n",
    "**Note:** This notebook will take about 10-15 minutes to execute all cells, depending on the compute configuration you have setup. Most of this time is spent handling the _telemetry_ data set, which contains about 8.7 million records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup our environment by importing required libraries\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Read csv file from URL directly\n",
    "import pandas as pd\n",
    "\n",
    "# For Azure blob storage access\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess\n",
    "\n",
    "# For creating some preliminary EDA plots.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from ggplot import *\n",
    "\n",
    "# Setup the pyspark environment\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# For logging model evaluation parameters back into the\n",
    "# AML Workbench run history plots.\n",
    "from azureml.logging import get_azureml_logger\n",
    "\n",
    "# Time the notebook execution. \n",
    "# This will only make sense if you \"Run All\" cells\n",
    "tic = time.time()\n",
    "\n",
    "logger = get_azureml_logger() # logger writes to AMLWorkbench runtime view\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Blob Storage Container\n",
    "\n",
    "We will be storing intermediate results for use between these Jupyter notebooks in an Azure Blob Storage container. Instructions for setting up your Azure Storage account are available within this link (https://docs.microsoft.com/en-us/azure/storage/blobs/storage-python-how-to-use-blob-storage). You will need to copy your account name and account key from the _Access Keys_ area in the portal into the following code block. These credentials will be reused in all four Jupyter notebooks. \n",
    "\n",
    "We will handle creating the containers and writing the data to these containers for each notebook. Further instructions for using Azure Blob storage with AML Workbench are available\n",
    "(https://github.com/Azure/ViennaDocs/blob/master/Documentation/UsingBlobForStorage.md).\n",
    "\n",
    "You will need to enter the **ACCOUNT_NAME** as well as the **ACCOUNT_KEY** in order to access Azure Blob storage account you have created. This notebook will create and store all the resulting data files in a blob container under this account. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Azure blob storage details here \n",
    "ACCOUNT_NAME = \"<your blob storage account name>\"\n",
    "\n",
    "# You can find the account key under the _Access Keys_ link in the \n",
    "# [Azure Portal](portal.azure.com) page for your Azure storage container.\n",
    "ACCOUNT_KEY = \"<your blob storage account key>\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# We will create this container to hold the results of executing this notebook.\n",
    "# If this container name already exists, we will use that instead, however\n",
    "# This notebook will ERASE ALL CONTENTS.\n",
    "CONTAINER_NAME = \"dataingestion\"\n",
    "\n",
    "# Connect to your blob service     \n",
    "az_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# Create a new container if necessary, otherwise you can use an existing container.\n",
    "# This command creates the container if it does not already exist. Else it does nothing.\n",
    "az_blob_service.create_container(CONTAINER_NAME, \n",
    "                                 fail_on_exist=False, \n",
    "                                 public_access=PublicAccess.Container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download simulated data sets\n",
    "We will be reusing the raw simulated data files from another tutorial. The notebook automatically downloads these files stored at [Microsoft/SQL-Server-R-Services-Samples GitHub site](https://github.com/Microsoft/SQL-Server-R-Services-Samples/tree/master/PredictiveMaintanenceModelingGuide/Data).\n",
    "\n",
    "The five data files are:\n",
    "\n",
    "    * machines.csv\n",
    "    * maint.csv\n",
    "    * errors.csv\n",
    "    * telemetry.csv\n",
    "    * failures.csv\n",
    "    \n",
    "This notebook does some preliminary cleanup, creates a summary graphic for each data set to verify the data download, and stores the resulting data sets in the Azure blob container created in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw data is stored on GitHub here:\n",
    "basedataurl = \"http://media.githubusercontent.com/media/Microsoft/SQL-Server-R-Services-Samples/master/PredictiveMaintanenceModelingGuide/Data/\"\n",
    "\n",
    "# We will store each of these data sets in blob storage in an \n",
    "# Azure Storage Container on your Azure subscription.\n",
    "# See https://github.com/Azure/ViennaDocs/blob/master/Documentation/UsingBlobForStorage.md\n",
    "# for details.\n",
    "\n",
    "# These file names detail which blob each files is stored under. \n",
    "MACH_DATA = 'machines_files.parquet'\n",
    "MAINT_DATA = 'maint_files.parquet'\n",
    "ERROR_DATA = 'errors_files.parquet'\n",
    "TELEMETRY_DATA = 'telemetry_files.parquet'\n",
    "FAILURE_DATA = 'failure_files.parquet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machines data set\n",
    "\n",
    "This simulation tracks a simulated set of 1000 machines over the course of a single year (2015). \n",
    "\n",
    "This data set includes information about each machine: Machine ID, model type and age (years in service). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data from the GitHub URL\n",
    "url = basedataurl + \"machines.csv\"\n",
    "\n",
    "machines=pd.read_csv(url, encoding='utf-8')\n",
    "\n",
    "print(machines.count())\n",
    "machines.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure plots a histogram of the machines age colored by the specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "_, bins, _ = plt.hist([machines.loc[machines['model'] == 'model1', 'age'],\n",
    "                       machines.loc[machines['model'] == 'model2', 'age'],\n",
    "                       machines.loc[machines['model'] == 'model3', 'age'],\n",
    "                       machines.loc[machines['model'] == 'model4', 'age']],\n",
    "                       20, stacked=True, label=['model1', 'model2', 'model3', 'model4'])\n",
    "plt.xlabel('Age (yrs)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows how long the collection of machines have been in service.\n",
    "\n",
    "Next, we convert the machines data to PySpark and store it in an Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data was read in using a Pandas data frame. We'll convert \n",
    "# it to pyspark to ensure it is in a Spark usable form for later \n",
    "# manipulations.\n",
    "mach_spark = spark.createDataFrame(machines, \n",
    "                                 verifySchema=False)\n",
    "\n",
    "# Write the Machine data set to intermediate storage\n",
    "mach_spark.write.mode('overwrite').parquet(MACH_DATA)\n",
    "\n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if MACH_DATA in blob.name:\n",
    "        az_blob_service.delete_blob(CONTAINER_NAME, blob.name)\n",
    "\n",
    "# upload the entire folder into blob storage\n",
    "for name in glob.iglob(MACH_DATA + '/*'):\n",
    "    print(os.path.abspath(name))\n",
    "    az_blob_service.create_blob_from_path(CONTAINER_NAME, name, name)\n",
    "\n",
    "print(\"Machines files saved!\")\n",
    "\n",
    "del machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors  data set\n",
    "\n",
    "The error log contains non-breaking errors recorded while the machine is still operational. These errors are not considered failures, though they may be predictive of a future failure event. The error datetime field is rounded to the closest hour since the telemetry data (loaded later) is collected on an hourly rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data from the GitHub URL\n",
    "url = basedataurl + \"errors.csv\"\n",
    "\n",
    "errors=pd.read_csv(url, encoding='utf-8')\n",
    "\n",
    "print(errors.count())\n",
    "errors.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following histogram details the distribution of the errors tracked in the log files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot to show structure\n",
    "ggplot(aes(x=\"errorID\"), errors) + geom_bar(fill=\"blue\", color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows how many errors occured in each of the 5 error classes over the entire year.\n",
    "\n",
    "Next, we convert the errors data to PySpark and store it in an Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data was read in using a Pandas data frame. We'll convert \n",
    "# it to pyspark to ensure it is in a Spark usable form for later \n",
    "# manipulations.\n",
    "error_spark = spark.createDataFrame(errors, \n",
    "                               verifySchema=False)\n",
    "\n",
    "# Write the Errors data set to intermediate storage\n",
    "error_spark.write.mode('overwrite').parquet(ERROR_DATA)\n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if ERROR_DATA in blob.name:\n",
    "        az_blob_service.delete_blob(CONTAINER_NAME, blob.name)\n",
    "\n",
    "# upload the entire folder into blob storage\n",
    "for name in glob.iglob(ERROR_DATA + '/*'):\n",
    "    print(os.path.abspath(name))\n",
    "    az_blob_service.create_blob_from_path(CONTAINER_NAME, name, name)\n",
    "\n",
    "print(\"Errors files saved!\")\n",
    "\n",
    "del errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenance data set\n",
    "\n",
    "The maintenance log contains both scheduled and unscheduled maintenance records. Scheduled maintenance corresponds with  regular inspection of components, unscheduled maintenance may arise from mechanical failure or other performance degradations. A failure record is generated for component replacement in the case  of either maintenance events. Because maintenance events can also be used to infer component life, the maintenance data has been collected over two years (2014, 2015) instead of only over the year of interest (2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data from the GitHub URL\n",
    "url = basedataurl + \"maint.csv\"\n",
    "maint=pd.read_csv(url, encoding='utf-8')\n",
    "\n",
    "print(maint.count())\n",
    "maint.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following histogram details the distribution of the maintenance records tracked in the maintenance log file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot to show structure\n",
    "ggplot(aes(x=\"comp\"), maint) + geom_bar(fill=\"blue\", color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows component replacements occured for each of the 4 component types over the entire year.\n",
    "\n",
    "Next, we convert the maintenance data to PySpark and store it in an Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data was read in using a Pandas data frame. We'll convert \n",
    "# it to pyspark to ensure it is in a Spark usable form for later \n",
    "# manipulations.\n",
    "maint_spark = spark.createDataFrame(maint, \n",
    "                              verifySchema=False)\n",
    "\n",
    "# Write the Maintenance data set to intermediate storage\n",
    "maint_spark.write.mode('overwrite').parquet(MAINT_DATA)\n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if MAINT_DATA in blob.name:\n",
    "        az_blob_service.delete_blob(CONTAINER_NAME, blob.name)\n",
    "\n",
    "# upload the entire folder into blob storage\n",
    "for name in glob.iglob(MAINT_DATA + '/*'):\n",
    "    print(os.path.abspath(name))\n",
    "    az_blob_service.create_blob_from_path(CONTAINER_NAME, name, name)\n",
    "\n",
    "print(\"Maintenance files saved!\")\n",
    "\n",
    "del maint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telemetry data set\n",
    "\n",
    "The telemetry time-series data consists of voltage, rotation, pressure, and vibration sensor measurements collected from each  machines in real time. The data is averaged over an hour and stored in the telemetry logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data from the GitHub URL\n",
    "url = basedataurl + \"telemetry.csv\"\n",
    "telemetry=pd.read_csv(url, encoding='utf-8')\n",
    "\n",
    "# handle missing values\n",
    "# define groups of features \n",
    "features_datetime = ['datetime']\n",
    "features_categorical = ['machineID']\n",
    "features_numeric = list(set(telemetry.columns) - set(features_datetime) - set(features_categorical))\n",
    "\n",
    "# Replace numeric NA with 0\n",
    "telemetry[features_numeric] = telemetry[features_numeric].fillna(0)\n",
    "\n",
    "# Replace categorical NA with 'Unknown'\n",
    "telemetry[features_categorical]  = telemetry[features_categorical].fillna(\"Unknown\")\n",
    "\n",
    "print(telemetry.count())\n",
    "telemetry.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than plot 8.7 million data points, this figure plots a month of voltage measurements for a single machine. This is representative of each feature repeated for every machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot to show structure\n",
    "plt_data = telemetry\n",
    "\n",
    "# format datetime field which comes in as string\n",
    "plt_data['datetime'] = pd.to_datetime(plt_data['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "plot_df = plt_data.loc[(plt_data['machineID'] == 1) &\n",
    "                        (plt_data['datetime'] > pd.to_datetime('2015-02-01')) &\n",
    "                        (plt_data['datetime'] < pd.to_datetime('2015-03-01'))]\n",
    "\n",
    "plt_data = pd.melt(plot_df, id_vars=['datetime', 'machineID'])\n",
    "\n",
    "ggplot(aes(x=\"datetime\", y=\"value\", color = \"variable\", group=\"variable\"), plt_data) +\\\n",
    "    scale_x_date(labels=date_format('%m-%d')) +\\\n",
    "    geom_line() +\\\n",
    "    facet_grid('variable', scales='free_y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows one month worth of telemetry sensor data for one machine. Each sensor is shown in it's own panel.\n",
    "\n",
    "Next, we convert the telemetry data to PySpark and store it in an Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data was read in using a Pandas data frame. We'll convert \n",
    "# it to pyspark to ensure it is in a Spark usable form for later \n",
    "# manipulations.\n",
    "\n",
    "# Explicitly converty datetime to string for spark DF.\n",
    "telemetry['datetime'] = telemetry['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Convert pandas to spark\n",
    "telemetry_spark = spark.createDataFrame(telemetry, verifySchema=False)\n",
    "\n",
    "# Write the telemetry data set to intermediate storage\n",
    "telemetry_spark.write.mode('overwrite').parquet(TELEMETRY_DATA)\n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if TELEMETRY_DATA in blob.name:\n",
    "        az_blob_service.delete_blob(CONTAINER_NAME, blob.name)\n",
    "\n",
    "# upload the entire folder into blob storage\n",
    "for name in glob.iglob(TELEMETRY_DATA + '/*'):\n",
    "    print(os.path.abspath(name))\n",
    "    az_blob_service.create_blob_from_path(CONTAINER_NAME, name, name)\n",
    "\n",
    "print(\"Telemetry files saved!\")\n",
    "\n",
    "# Clear memory\n",
    "del telemetry\n",
    "del plt_data\n",
    "del plot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failures data set\n",
    "\n",
    "Failures correspond to component replacements within the maintenance log. Each record contains the Machine ID, component type, and replacement datetime. These records will be used to create the machine learning labels we will be trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data from the GitHub URL\n",
    "url = basedataurl + \"failures.csv\"\n",
    "\n",
    "failures=pd.read_csv(url, encoding='utf-8')\n",
    "\n",
    "print(failures.count())\n",
    "failures.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following histogram details the distribution of the failure records obtained from failure log. This log was built originally from component replacements the maintenance log file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot failures\n",
    "ggplot(aes(x=\"failure\"), failures) + geom_bar(fill=\"blue\", color=\"black\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows failure related replacements occured for each of the 4 component types over the entire year.\n",
    "\n",
    "Next, we convert the maintenance data to PySpark and store it in an Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data was read in using a Pandas data frame. We'll convert \n",
    "# it to pyspark to ensure it is in a Spark usable form for later \n",
    "# manipulations.\n",
    "failures_spark = spark.createDataFrame(failures, \n",
    "                                 verifySchema=False)\n",
    "\n",
    "# Write the failures data set to intermediate storage\n",
    "failures_spark.write.mode('overwrite').parquet(FAILURE_DATA)\n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if FAILURE_DATA in blob.name:\n",
    "        az_blob_service.delete_blob(CONTAINER_NAME, blob.name)\n",
    "\n",
    "# upload the entire folder into blob storage\n",
    "for name in glob.iglob(FAILURE_DATA + '/*'):\n",
    "    print(os.path.abspath(name))\n",
    "    az_blob_service.create_blob_from_path(CONTAINER_NAME, name, name)\n",
    "\n",
    "print(\"Failure files saved!\")\n",
    "\n",
    "# Time the notebook execution. \n",
    "# This will only make sense if you \"Run All\" cells\n",
    "toc = time.time()\n",
    "print(\"Full run took %.2f minutes\" % ((toc - tic)/60))\n",
    "\n",
    "logger.log(\"Data Ingestion Run time\", ((toc - tic)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have now downloaded the required data files, and saved them into an Azure Blob storage container for use in the next step. The `Code\\2_feature_engineering.ipynb` Jupyter notebook will read these data files in from Azure blob and generate the modeling features for out predictive maintnance machine learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdmScenarioTest dsvmDocker",
   "language": "python",
   "name": "pdmscenariotest_dsvmdocker"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

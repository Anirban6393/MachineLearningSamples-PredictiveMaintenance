{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Feature Engineering\n",
    "\n",
    "Feature engineering combines the different data sources together to create a single data set of features (variables) that can be used to infer a machines's health condition over time. \n",
    "\n",
    "In this notebook, we will load the data stored in Azure Blob containers in the previous **Data Ingestion** notebook (`Code/data_ingestion.ipynb`). The note book uses several feature engineering methods to create a data set for use in our predictive maintenance machine learning solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, unix_timestamp, round\n",
    "from pyspark.sql.functions import datediff\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from Azure Blob storage container\n",
    "\n",
    "We have previously downloaded and stored the following data in an Azure blob storage container:\n",
    "\n",
    "  * Machines: Features differentiating each machine. For example age and model.\n",
    "  * Error: The log of non-critical errors. These errors may still indicate an impending component failure.\n",
    "  * Maint: Machine maintenance history detailing component replacement or regular maintenance activities withe the date of replacement.\n",
    "  * Telemetry: The operating conditions of a machine e.g. data collected from sensors.\n",
    "  * Failure history: The failure history of a machine or component within the machine.\n",
    "\n",
    "We'll load these files from blob, and create our analysis data set here. We'll write this data set back into a new blob container to use in our model building and evaluation notebook later. \n",
    "\n",
    "Since the Azure Blob storage account name and account key are not passed between notebooks, you'll need to provide those here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enter your Azure blob storage details here \n",
    "ACCOUNT_NAME = \"pdmamlworkbench\"   ## \"<your blob storage account name>\"\n",
    "\n",
    "# You can find the account key under the _Access Keys_ link in the \n",
    "# [Azure Portal](portal.azure.com) page for your Azure storage container.\n",
    "ACCOUNT_KEY = \"O5uLzNKX7o+ZHFXtHDyS87SIev9QHlkdX2IhIbxYwhRo7sA9zp45HOOFFttUp4r0LyWCcLQ0cCA7l+e8Ct3Yew==\" ## \"<account key>\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# The data from the Data Aquisition note book is stored in the dataingestion container.\n",
    "CONTAINER_NAME = \"dataingestion\"\n",
    "\n",
    "# Connect to your blob service     \n",
    "my_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machines data set\n",
    "\n",
    "Load the machines data set from your Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a local path where to store the results later.\n",
    "LOCAL_DIRECT = 'dataingestion_mach_result.parquet'\n",
    "if not os.path.exists(LOCAL_DIRECT):\n",
    "    os.makedirs(LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# define your blob service     \n",
    "my_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in my_service.list_blobs(CONTAINER_NAME):\n",
    "    if 'machines_files.parquet' in blob.name:\n",
    "        local_file = os.path.join(LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        my_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "machines = spark.read.parquet(LOCAL_DIRECT)\n",
    "\n",
    "print(machines.count())\n",
    "machines.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors data set\n",
    "\n",
    "Load the errors data set from your Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the previous created final dataset into the workspace\n",
    "\n",
    "# create a local path where to store the results later.\n",
    "LOCAL_DIRECT = 'dataingestion_err_result.parquet'\n",
    "if not os.path.exists(LOCAL_DIRECT):\n",
    "    os.makedirs(LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# define your blob service     \n",
    "my_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in my_service.list_blobs(CONTAINER_NAME):\n",
    "    if 'errors_files.parquet' in blob.name:\n",
    "        local_file = os.path.join(LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        my_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "errors = spark.read.parquet('dataingestion_err_result.parquet')\n",
    "\n",
    "print(errors.count())\n",
    "errors.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maintenance data set\n",
    "\n",
    "Load the maintenance data set from your Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a local path where to store the results later.\n",
    "LOCAL_DIRECT = 'dataingestion_maint_result.parquet'\n",
    "if not os.path.exists(LOCAL_DIRECT):\n",
    "    os.makedirs(LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# define your blob service     \n",
    "my_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in my_service.list_blobs(CONTAINER_NAME):\n",
    "    if 'maint_files.parquet' in blob.name:\n",
    "        local_file = os.path.join(LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        my_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "maint = spark.read.parquet('dataingestion_maint_result.parquet')\n",
    "\n",
    "print(maint.count())\n",
    "maint.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telemetry\n",
    "\n",
    "Load the telemetry data set from your Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a local path where to store the results later.\n",
    "LOCAL_DIRECT = 'dataingestion_tel_result.parquet'\n",
    "if not os.path.exists(LOCAL_DIRECT):\n",
    "    os.makedirs(LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# define your blob service     \n",
    "my_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in my_service.list_blobs(CONTAINER_NAME):\n",
    "    if 'telemetry_files.parquet' in blob.name:\n",
    "        local_file = os.path.join(LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        my_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "telemetry = spark.read.parquet('dataingestion_tel_result.parquet')\n",
    "\n",
    "print(telemetry.count())\n",
    "telemetry.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failures data set\n",
    "\n",
    "Load the failures data set from your Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a local path where to store the results later.\n",
    "LOCAL_DIRECT = 'dataingestion_fail_result.parquet'\n",
    "if not os.path.exists(LOCAL_DIRECT):\n",
    "    os.makedirs(LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# define your blob service     \n",
    "my_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in my_service.list_blobs(CONTAINER_NAME):\n",
    "    if 'failure_files.parquet' in blob.name:\n",
    "        local_file = os.path.join(LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        my_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "failures = spark.read.parquet('dataingestion_fail_result.parquet')\n",
    "\n",
    "print(failures.count())\n",
    "failures.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering \n",
    "\n",
    "Feature engineering combines the different data sources together to create a single data set of features (variables) that can be used to infer a machines's health condition over time. The ultimate goal is to generate a single record for each time unit for each asset combining its features and labels to be fed into the machine learning algorithm. In order to prepare that clean final data set, some pre-processing steps should be taken. First step is to divide the duration of data collection into time units where each record belongs to a time unit for an asset.\n",
    "\n",
    "The measurement unit for time can be in seconds, minutes, hours, days, months, cycles, miles or transactions depending on the efficiency of data preparation and the changes observed in the conditions of the asset from a time unit to the other or other factors specific to the domain. In other words, the time unit does not have to be the same as the frequency of data collection as in many cases data may not show any difference from one unit to the other. For example, if temperature values were being collected every 10 seconds, picking a time unit of 10 seconds for the whole analysis inflates the number of examples without providing any additional information. Better strategy would be to use average over an hour as an example.\n",
    "\n",
    "### Rolling aggregates\n",
    "\n",
    "For each record of an asset, we pick a rolling window of size \"W\" which is the number of units of time that we would like to compute historical aggregates for. We then compute rolling aggregate features using the W periods before the date of that record. Some example rolling aggregates can be rolling counts, means, standard deviations, outliers based on standard deviations, CUSUM measures, minimum and maximum values for the window. Another interesting technique is to capture trend changes, spikes and level changes using algorithms that detect anomalies in data using anomaly detection algorithms.\n",
    "\n",
    "### Lag features\n",
    "As mentioned earlier, in predictive maintenance, historical data usually comes with timestamps indicating the time of collection for each piece of data. There are many ways of creating features from the data that comes with timestamped data. In this section, we discuss some of these methods used for predictive maintenance. However, we are not limited by these methods alone. Since feature engineering is considered to be one of the most creative areas of predictive modeling, there could be many other ways to create features. Here, we provide some general techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telemetry features\n",
    "\n",
    "Because the telemetry data set is the largest time series data we have, we start feature engineering here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rolling mean\n",
    "# Temporary storage for rolling means\n",
    "tel_mean = telemetry\n",
    "\n",
    "# Which features are we interested in telemetry data set\n",
    "rolling_features = ['volt','rotate', 'pressure', 'vibration']\n",
    "               \n",
    "# We choose two windows for our rolling windows 3hrs, 24 hrs\n",
    "lags = [3,24]\n",
    "\n",
    "for lag_n in lags:\n",
    "    wSpec = Window.partitionBy('machineID').orderBy('datetime').rowsBetween(1-lag_n, 0)\n",
    "    for col_name in rolling_features:\n",
    "        tel_mean = tel_mean.withColumn(col_name+'_rollingmean_'+str(lag_n), F.avg(col(col_name)).over(wSpec))\n",
    "        print(\"Lag = %d, Column = %s\" % (lag_n, col_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this rolling window process for the standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rolling standard deviation\n",
    "# Temporary storage for rolling means\n",
    "tel_sd = telemetry\n",
    "\n",
    "for lag_n in lags:\n",
    "    wSpec = Window.partitionBy('machineID').orderBy('datetime').rowsBetween(1-lag_n, 0)\n",
    "    for col_name in rolling_features:\n",
    "        tel_sd = tel_sd.withColumn(col_name+'_rollingstd_'+str(lag_n), F.stddev(col(col_name)).over(wSpec))\n",
    "        print(\"Lag = %d, Column = %s\" % (lag_n, col_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample every 3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+--------------------+\n",
      "|            datetime|machineID|volt_rollingmean_3|rotate_rollingmean_3|pressure_rollingmean_3|vibration_rollingmean_3|volt_rollingmean_24|rotate_rollingmean_24|pressure_rollingmean_24|vibration_rollingmean_24|        dt_truncated|\n",
      "+--------------------+---------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+--------------------+\n",
      "|2015-01-01 06:00:...|        1|  151.919998705647|    530.813577555042|      101.788175260076|       49.6040134898504|   151.919998705647|     530.813577555042|       101.788175260076|        49.6040134898504|2015-01-01 06:00:...|\n",
      "|2015-01-01 07:00:...|        1|  163.220999901059|    533.168554937213|      107.522092379665|       45.5599594825861|   163.220999901059|     533.168554937213|       107.522092379665|        45.5599594825861|2015-01-01 06:00:...|\n",
      "|2015-01-01 08:00:...|        1|  157.784940482728|   507.4726186267447|    107.61038313093032|     44.406537539917935|   157.784940482728|    507.4726186267447|     107.61038313093032|      44.406537539917935|2015-01-01 09:00:...|\n",
      "|2015-01-01 09:00:...|        1|166.98846119831367|  498.35808960356803|    109.77559711816201|      40.48777554150487| 163.22134557514698|    506.4719615914365|      107.7787416536405|       42.76683502859125|2015-01-01 09:00:...|\n",
      "|2015-01-01 10:00:...|        1|168.99588637326565|  443.71711592888465|    107.87469621802067|     40.471767062267894| 166.68593178438297|   479.49769153221604|     107.73365468267839|      42.507044030395186|2015-01-01 09:00:...|\n",
      "+--------------------+---------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+\n",
      "|machineID|        dt_truncated|volt_rollingmean_3|rotate_rollingmean_3|pressure_rollingmean_3|vibration_rollingmean_3|volt_rollingmean_24|rotate_rollingmean_24|pressure_rollingmean_24|vibration_rollingmean_24|\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+\n",
      "|        1|2015-01-01 06:00:...|157.57049930335302|   531.9910662461275|     104.6551338198705|     47.581986486218256| 157.57049930335302|    531.9910662461275|      104.6551338198705|      47.581986486218256|\n",
      "|        1|2015-01-01 09:00:...| 164.5897626847691|  483.18260805306574|    108.42022548903766|      41.78869338123024| 162.56407261408597|   497.81409058346577|     107.70759315574973|       43.22680553296812|\n",
      "|        1|2015-01-01 12:00:...|166.62829412128102|   469.5738785281323|     96.63276976305065|      40.86484505715328|  164.5763498606411|     484.613651237679|     103.04446537517121|      42.151638488191416|\n",
      "|        1|2015-01-01 15:00:...|171.06779818151801|   488.4581387657104|     99.72635122879298|      43.38640212756743| 166.52268391941996|   485.51093874805173|     102.02220291528766|      42.503124493788015|\n",
      "|        1|2015-01-01 18:00:...|178.10604170433922|  433.81353435738373|     96.08070959588888|     42.261642612158205| 169.32568592452287|     473.620841575823|     100.66056456580509|       42.43234484565859|\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2921000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tel_mean rolling mean\n",
    "# 3 hours = 10800 seconds  \n",
    "time_val = 3 * 60 * 60\n",
    "\n",
    "# I think this is grabbing datetime from the tel_sd data set, which is equivalent to telemetry\n",
    "dt_truncated = ((round(unix_timestamp(col(\"datetime\")) / time_val) * time_val).cast(\"timestamp\"))\n",
    "\n",
    "tel_mean_resampled = tel_mean.withColumn(\"dt_truncated\", dt_truncated).drop('volt', 'rotate', 'pressure', 'vibration')\n",
    "\n",
    "tel_mean_resampled1 = (tel_mean_resampled.groupBy(\"machineID\",\"dt_truncated\")\n",
    "                               .agg(F.mean('volt_rollingmean_3').alias('volt_rollingmean_3'),\n",
    "                                    F.mean('rotate_rollingmean_3').alias('rotate_rollingmean_3'), \n",
    "                                    F.mean('pressure_rollingmean_3').alias('pressure_rollingmean_3'), \n",
    "                                    F.mean('vibration_rollingmean_3').alias('vibration_rollingmean_3'), \n",
    "                                    F.mean('volt_rollingmean_24').alias('volt_rollingmean_24'),\n",
    "                                    F.mean('rotate_rollingmean_24').alias('rotate_rollingmean_24'), \n",
    "                                    F.mean('pressure_rollingmean_24').alias('pressure_rollingmean_24'), \n",
    "                                    F.mean('vibration_rollingmean_24').alias('vibration_rollingmean_24')))\n",
    "\n",
    "tel_mean_resampled1.where((col(\"machineID\") == 1)).toPandas().head(5)\n",
    "tel_mean_resampled1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+--------------------+\n",
      "|            datetime|machineID| volt_rollingstd_3|rotate_rollingstd_3|pressure_rollingstd_3|vibration_rollingstd_3|volt_rollingstd_24|rotate_rollingstd_24|pressure_rollingstd_24|vibration_rollingstd_24|        dt_truncated|\n",
      "+--------------------+---------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+--------------------+\n",
      "|2015-01-01 06:00:...|      148|               0.0|                0.0|                  0.0|                   0.0|               0.0|                 0.0|                   0.0|                    0.0|2015-01-01 06:00:...|\n",
      "|2015-01-01 07:00:...|      148|19.182967495343476|  1.603237700069381|   1.4943454094246378|     7.478704304240883|19.182967495343476|   1.603237700069381|    1.4943454094246378|      7.478704304240883|2015-01-01 06:00:...|\n",
      "|2015-01-01 08:00:...|      148|13.670489782652124|  48.77869800074968|   1.3007593012349516|     5.290696784240286|13.670489782652124|   48.77869800074968|    1.3007593012349516|      5.290696784240286|2015-01-01 09:00:...|\n",
      "|2015-01-01 09:00:...|      148|14.798006014623844|  42.83133996381171|   11.724128378059122|    11.943608173160483|13.442479552969818|   40.66224888480563|    10.003239005412144|      9.774548480484773|2015-01-01 09:00:...|\n",
      "|2015-01-01 10:00:...|      148| 9.294624874529687|  59.37220777144211|    9.576618443930311|     8.869202720295732|12.028183455757103|  45.548330823467516|     8.798581987295949|      8.619991997624002|2015-01-01 09:00:...|\n",
      "+--------------------+---------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+\n",
      "|machineID|        dt_truncated| volt_rollingstd_3|rotate_rollingstd_3|pressure_rollingstd_3|vibration_rollingstd_3|volt_rollingstd_24|rotate_rollingstd_24|pressure_rollingstd_24|vibration_rollingstd_24|\n",
      "+---------+--------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+\n",
      "|      148|2015-01-17 09:00:...|   9.9990071169557|   68.8542413174133|   7.6452303107269985|     5.610868339008934|14.025390258618202|   60.15462008098817|      8.41733644602634|      5.246216020228252|\n",
      "|      148|2015-01-31 12:00:...|6.8015077309107985|  26.53854876709855|   11.986579171195073|     3.225815866176598| 10.97687093387308|  49.416991532803166|     13.17302250200619|      5.054396183604651|\n",
      "|      463|2015-01-23 03:00:...| 9.309113564896231| 17.107664630056238|   16.161941208553376|     4.550825608227414|14.962506523541771|   52.73851112443531|     14.56600450621893|     3.9760104740144677|\n",
      "|      471|2015-01-03 00:00:...|  16.1477457489993|  64.35585597693857|   14.084073152243834|    3.7740854627276668| 15.11174527581719|   54.75517617088082|    10.364187948595807|      4.876406090734396|\n",
      "|      471|2015-01-03 09:00:...|16.367127392879947| 32.420371220085066|    9.750392019740522|     4.176415255139442|15.704205546681289|  54.083613516090566|    12.030984787472377|      4.672465133789742|\n",
      "+---------+--------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2921000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tel_sd rolling sd\n",
    "tel_sd_resampled = (tel_sd.withColumn(\"dt_truncated\", dt_truncated).drop('volt', 'rotate', 'pressure', 'vibration')\n",
    "                        .fillna(0))\n",
    "\n",
    "tel_sd_resampled1 = (tel_sd_resampled.groupBy(\"machineID\",\"dt_truncated\")\n",
    "                               .agg(F.sd('volt_rollingstd_3').alias('volt_rollingstd_3'),\n",
    "                                    F.sd('rotate_rollingstd_3').alias('rotate_rollingstd_3'), \n",
    "                                    F.sd('pressure_rollingstd_3').alias('pressure_rollingstd_3'), \n",
    "                                    F.sd('vibration_rollingstd_3').alias('vibration_rollingstd_3'), \n",
    "                                    F.sd('volt_rollingstd_24').alias('volt_rollingstd_24'),\n",
    "                                    F.sd('rotate_rollingstd_24').alias('rotate_rollingstd_24'), \n",
    "                                    F.sd('pressure_rollingstd_24').alias('pressure_rollingstd_24'), \n",
    "                                    F.sd('vibration_rollingstd_24').alias('vibration_rollingstd_24')))\n",
    "tel_sd_resampled1.show(5)\n",
    "tel_sd_resampled1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag features from Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+-------+------+------+------+------+------+\n",
      "|machineID|datetime             |errorID|error1|error2|error3|error4|error5|\n",
      "+---------+---------------------+-------+------+------+------+------+------+\n",
      "|14       |2015-04-01 13:00:00.0|error1 |1     |null  |null  |null  |null  |\n",
      "|17       |2015-04-07 06:00:00.0|error5 |null  |null  |null  |null  |1     |\n",
      "|29       |2015-07-22 06:00:00.0|error2 |null  |1     |null  |null  |null  |\n",
      "|63       |2015-11-09 06:00:00.0|error1 |1     |null  |null  |null  |null  |\n",
      "|71       |2015-03-11 06:00:00.0|error5 |null  |null  |null  |null  |1     |\n",
      "+---------+---------------------+-------+------+------+------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11967, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a column for each errorID \n",
    "error1 = errors.groupBy(\"machineID\",\"datetime\",\"errorID\").pivot('errorID').agg(F.count('machineID').alias('dummy'))\n",
    "\n",
    "# remove the column called errorID and fill in missing values\n",
    "error2 = error1.drop('errorID').fillna(0)\n",
    "\n",
    "# combine errors for a given machine in a given hour\n",
    "error3 = (error2.groupBy(\"machineID\",\"datetime\")\n",
    "                .agg(F.sum('error1').alias('error1sum'), \n",
    "                     F.sum('error2').alias('error2sum'), \n",
    "                     F.sum('error3').alias('error3sum'), \n",
    "                     F.sum('error4').alias('error4sum'), \n",
    "                     F.sum('error5').alias('error5sum')))\n",
    "\n",
    "error3.toPandas().head(5)\n",
    "error3.count(), len(error3.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the same number of rows in error as in telemetry.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+---------+---------+---------+---------+---------+\n",
      "|datetime             |machineID|error1sum|error2sum|error3sum|error4sum|error5sum|\n",
      "+---------------------+---------+---------+---------+---------+---------+---------+\n",
      "|2015-01-01 06:00:00.0|1        |null     |null     |null     |null     |null     |\n",
      "|2015-01-01 07:00:00.0|1        |null     |null     |null     |null     |null     |\n",
      "|2015-01-01 08:00:00.0|1        |null     |null     |null     |null     |null     |\n",
      "|2015-01-01 09:00:00.0|1        |null     |null     |null     |null     |null     |\n",
      "|2015-01-01 10:00:00.0|1        |null     |null     |null     |null     |null     |\n",
      "+---------------------+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8761000, 7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the telemetry data with errors\n",
    "error_count = (telemetry.join(error3, ((telemetry['machineID'] == error3['machineID']) \n",
    "                                  & (telemetry['datetime'] == error3['datetime'])), \"left\")\n",
    "               .drop('volt', 'rotate', 'pressure', 'vibration').drop(error3.machineID).drop(error3.datetime))\n",
    "\n",
    "# fill in missing value\n",
    "error_count1 = error_count.fillna(0)\n",
    "\n",
    "error_count1.toPandas().show(5)\n",
    "error_count1.count(), len(error_count1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag = 24, Column = error1sum\n",
      "Lag = 24, Column = error2sum\n",
      "Lag = 24, Column = error3sum\n",
      "Lag = 24, Column = error4sum\n",
      "Lag = 24, Column = error5sum\n"
     ]
    }
   ],
   "source": [
    "rolling_features1 = ['error1sum','error2sum', 'error3sum', 'error4sum', 'error5sum']\n",
    "               \n",
    "# lag window 24 hrs\n",
    "lags = [24]\n",
    "\n",
    "# rolling mean\n",
    "err_mean = error_count1\n",
    "\n",
    "for lag_n in lags:\n",
    "    wSpec = Window.partitionBy('machineID').orderBy('datetime').rowsBetween(1-lag_n, 0)\n",
    "    for col_name in rolling_features1:\n",
    "        err_mean = err_mean.withColumn(col_name+'_rollingmean_'+str(lag_n), F.avg(col(col_name)).over(wSpec))\n",
    "        print(\"Lag = %d, Column = %s\" % (lag_n, col_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample to every 3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------------------+------------------------+------------------------+------------------------+------------------------+--------------------+\n",
      "|            datetime|machineID|error1sum_rollingmean_24|error2sum_rollingmean_24|error3sum_rollingmean_24|error4sum_rollingmean_24|error5sum_rollingmean_24|        dt_truncated|\n",
      "+--------------------+---------+------------------------+------------------------+------------------------+------------------------+------------------------+--------------------+\n",
      "|2015-01-01 06:00:...|      148|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|2015-01-01 06:00:...|\n",
      "|2015-01-01 07:00:...|      148|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|2015-01-01 06:00:...|\n",
      "|2015-01-01 08:00:...|      148|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|2015-01-01 09:00:...|\n",
      "|2015-01-01 09:00:...|      148|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|2015-01-01 09:00:...|\n",
      "|2015-01-01 10:00:...|      148|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|2015-01-01 09:00:...|\n",
      "+--------------------+---------+------------------------+------------------------+------------------------+------------------------+------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+--------------------+------------------------+------------------------+------------------------+------------------------+------------------------+\n",
      "|machineID|        dt_truncated|error1sum_rollingmean_24|error2sum_rollingmean_24|error3sum_rollingmean_24|error4sum_rollingmean_24|error5sum_rollingmean_24|\n",
      "+---------+--------------------+------------------------+------------------------+------------------------+------------------------+------------------------+\n",
      "|      148|2015-01-17 09:00:...|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|\n",
      "|      148|2015-01-31 12:00:...|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|\n",
      "|      463|2015-01-23 03:00:...|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|\n",
      "|      471|2015-01-03 00:00:...|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|\n",
      "|      471|2015-01-03 09:00:...|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|\n",
      "+---------+--------------------+------------------------+------------------------+------------------------+------------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2921000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_truncated = ((round(unix_timestamp(col(\"datetime\")) / time_val) * time_val)\n",
    "    .cast(\"timestamp\"))\n",
    "\n",
    "err_mean_resampled = (err_mean.withColumn(\"dt_truncated\", dt_truncated)\n",
    "                    .drop('error1sum', 'error2sum', 'error3sum', 'error4sum', 'error5sum').fillna(0))\n",
    "\n",
    "err_mean_resampled1 = (err_mean_resampled.groupBy(\"machineID\",\"dt_truncated\")\n",
    "                               .agg(F.mean('error1sum_rollingmean_24').alias('error1sum_rollingmean_24'), \n",
    "                                    F.mean('error2sum_rollingmean_24').alias('error2sum_rollingmean_24'), \n",
    "                                    F.mean('error3sum_rollingmean_24').alias('error3sum_rollingmean_24'), \n",
    "                                    F.mean('error4sum_rollingmean_24').alias('error4sum_rollingmean_24'), \n",
    "                                    F.mean('error5sum_rollingmean_24').alias('error5sum_rollingmean_24')))\n",
    "err_mean_resampled1.toPandas().head(5)\n",
    "err_mean_resampled1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Days since last replacement from maintenance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+-----+-----+-----+-----+-----+\n",
      "|machineID|datetime             |comp |comp1|comp2|comp3|comp4|\n",
      "+---------+---------------------+-----+-----+-----+-----+-----+\n",
      "|8        |2015-07-03 06:00:00.0|comp3|null |null |1    |null |\n",
      "|8        |2015-08-02 06:00:00.0|comp2|null |1    |null |null |\n",
      "|24       |2015-04-20 06:00:00.0|comp3|null |null |1    |null |\n",
      "|35       |2015-05-05 06:00:00.0|comp2|null |1    |null |null |\n",
      "|38       |2015-04-02 06:00:00.0|comp4|null |null |null |1    |\n",
      "+---------+---------------------+-----+-----+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32592, 8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a column for each comp \n",
    "maint1 = maint.groupBy(\"machineID\",\"datetime\",\"comp\").pivot('comp').agg(F.count('machineID').alias('dummy'))\n",
    "\n",
    "# remove the column called comp and fill in missing values\n",
    "maint2 = maint1.drop('comp').fillna(0)\n",
    "\n",
    "# combine maintenance for a given machine in a given hour\n",
    "maint3 = (maint2.groupBy(\"machineID\",\"datetime\").agg(F.sum('comp1').alias('comp1sum'), \n",
    "                                                    F.sum('comp2').alias('comp2sum'), \n",
    "                                                    F.sum('comp3').alias('comp3sum'),\n",
    "                                                    F.sum('comp4').alias('comp4sum')))\n",
    "maint3.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Days since last replacement for component-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------+\n",
      "|        datetime_tel|machineID|sincelastcomp1|\n",
      "+--------------------+---------+--------------+\n",
      "|2015-01-01 06:00:...|        1|           109|\n",
      "|2015-01-01 07:00:...|        1|           109|\n",
      "|2015-01-01 08:00:...|        1|           109|\n",
      "|2015-01-01 09:00:...|        1|           109|\n",
      "|2015-01-01 10:00:...|        1|           109|\n",
      "+--------------------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_maint_comp1 = (maint3.where((col(\"comp1sum\") == '1')).withColumnRenamed('datetime','datetime_maint')\n",
    "                           .drop('comp2sum', 'comp3sum', 'comp4sum'))\n",
    "\n",
    "test_tel_comp1 = (telemetry.withColumnRenamed('datetime','datetime_tel')\n",
    "                  .drop(telemetry.volt).drop(telemetry.rotate).drop(telemetry.pressure).drop(telemetry.vibration))\n",
    "\n",
    "test_maint_tel_comp1 = test_tel_comp1.join(test_maint_comp1, ((test_tel_comp1['machineID']==\n",
    "                                                               test_maint_comp1['machineID']) \n",
    "                                            & (test_tel_comp1['datetime_tel'] > test_maint_comp1['datetime_maint']) \n",
    "                                            & (test_maint_comp1['comp1sum'] == '1'))).drop(test_maint_comp1.machineID)\n",
    "\n",
    "comp1 = (test_maint_tel_comp1.withColumn(\"sincelastcomp1\", \n",
    "              datediff(test_maint_tel_comp1.datetime_tel, test_maint_tel_comp1.datetime_maint))\n",
    "              .drop(test_maint_tel_comp1.datetime_maint).drop(test_maint_tel_comp1.comp1sum))\n",
    "\n",
    "comp1.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Days since last replacement for component-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------+\n",
      "|        datetime_tel|machineID|sincelastcomp2|\n",
      "+--------------------+---------+--------------+\n",
      "|2015-01-01 06:00:...|        1|           109|\n",
      "|2015-01-01 07:00:...|        1|           109|\n",
      "|2015-01-01 08:00:...|        1|           109|\n",
      "|2015-01-01 09:00:...|        1|           109|\n",
      "|2015-01-01 10:00:...|        1|           109|\n",
      "+--------------------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_maint_comp2 = (maint3.where(col(\"comp2sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n",
    "                         .drop('comp1sum', 'comp3sum', 'comp4sum'))\n",
    "\n",
    "test_tel_comp2 = (telemetry.withColumnRenamed('datetime','datetime_tel')\n",
    "                          .drop(telemetry.volt).drop(telemetry.rotate).drop(telemetry.pressure)\n",
    "                          .drop(telemetry.vibration))\n",
    "\n",
    "test_maint_tel_comp2 = (test_tel_comp2.join(test_maint_comp2, ((test_tel_comp2['machineID']==\n",
    "                                                                test_maint_comp2['machineID']) \n",
    "                                        & (test_tel_comp2['datetime_tel'] > test_maint_comp2['datetime_maint']) \n",
    "                                        & (test_maint_comp2['comp2sum'] == '1') \n",
    "                                           )).drop(test_maint_comp2.machineID))\n",
    "\n",
    "comp2 = (test_maint_tel_comp2.withColumn(\"sincelastcomp2\", \n",
    "              datediff(test_maint_tel_comp2.datetime_tel, test_maint_tel_comp2.datetime_maint))\n",
    "              .drop(test_maint_tel_comp2.datetime_maint).drop(test_maint_tel_comp2.comp2sum))\n",
    "comp2.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Days since last replacement for component-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------+\n",
      "|        datetime_tel|machineID|sincelastcomp3|\n",
      "+--------------------+---------+--------------+\n",
      "|2015-01-01 06:00:...|        1|            49|\n",
      "|2015-01-01 07:00:...|        1|            49|\n",
      "|2015-01-01 08:00:...|        1|            49|\n",
      "|2015-01-01 09:00:...|        1|            49|\n",
      "|2015-01-01 10:00:...|        1|            49|\n",
      "+--------------------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_maint_comp3 = (maint3.where(col(\"comp3sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n",
    "                          .drop('comp1sum', 'comp2sum', 'comp4sum'))\n",
    "\n",
    "test_tel_comp3 = (telemetry.withColumnRenamed('datetime','datetime_tel')\n",
    "                    .drop(telemetry.volt).drop(telemetry.rotate).drop(telemetry.pressure).drop(telemetry.vibration))\n",
    "\n",
    "test_maint_tel_comp3 = test_tel_comp3.join(test_maint_comp3, ((test_tel_comp3['machineID']==\n",
    "                                                               test_maint_comp3['machineID']) \n",
    "                                        & (test_tel_comp3['datetime_tel'] > test_maint_comp3['datetime_maint']) \n",
    "                                        & (test_maint_comp3['comp3sum'] == '1') \n",
    "                                           )).drop(test_maint_comp3.machineID)\n",
    "\n",
    "comp3 = (test_maint_tel_comp3.withColumn(\"sincelastcomp3\", \n",
    "              datediff(test_maint_tel_comp3.datetime_tel, test_maint_tel_comp3.datetime_maint))\n",
    "              .drop(test_maint_tel_comp3.datetime_maint).drop(test_maint_tel_comp3.comp3sum))\n",
    "comp3.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Days since last replacement for component-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------+\n",
      "|        datetime_tel|machineID|sincelastcomp4|\n",
      "+--------------------+---------+--------------+\n",
      "|2015-01-01 06:00:...|        1|           184|\n",
      "|2015-01-01 07:00:...|        1|           184|\n",
      "|2015-01-01 08:00:...|        1|           184|\n",
      "|2015-01-01 09:00:...|        1|           184|\n",
      "|2015-01-01 10:00:...|        1|           184|\n",
      "+--------------------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_maint_comp4 = (maint3.where(col(\"comp4sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n",
    "                         .drop('comp1sum', 'comp2sum', 'comp3sum'))\n",
    "\n",
    "test_tel_comp4 = (telemetry.withColumnRenamed('datetime','datetime_tel')\n",
    "                  .drop(telemetry.volt).drop(telemetry.rotate).drop(telemetry.pressure).drop(telemetry.vibration))\n",
    "\n",
    "test_maint_tel_comp4 = test_tel_comp4.join(test_maint_comp4, ((test_tel_comp4['machineID']==\n",
    "                                                               test_maint_comp4['machineID']) \n",
    "                                        & (test_tel_comp4['datetime_tel'] > test_maint_comp4['datetime_maint']) \n",
    "                                        & (test_maint_comp4['comp4sum'] == '1'))).drop(test_maint_comp4.machineID)\n",
    "\n",
    "comp4 = (test_maint_tel_comp4.withColumn(\"sincelastcomp4\", \n",
    "              datediff(test_maint_tel_comp4.datetime_tel, test_maint_tel_comp4.datetime_maint))\n",
    "              .drop(test_maint_tel_comp4.datetime_maint).drop(test_maint_tel_comp4.comp4sum))\n",
    "comp4.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Combine comp1, comp2, comp3, comp4 to generate the maintenance feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# left join comp1 with (comp2, comp3, comp4) \n",
    "# left join comp2 with (comp3, comp4) \n",
    "# left join comp3, comp4 \n",
    "comp3_4 = (comp3.join(comp4, ((comp3['machineID'] == comp4['machineID']) \n",
    "                                  & (comp3['datetime_tel'] == comp4['datetime_tel'])), \"left\")\n",
    "                                  .drop(comp4.machineID).drop(comp4.datetime_tel))comp2_3_4 = (comp2.join(comp3_4, ((comp2['machineID'] == comp3_4['machineID']) \n",
    "                                  & (comp2['datetime_tel'] == comp3_4['datetime_tel'])), \"left\")\n",
    "                                  .drop(comp3_4.machineID).drop(comp3_4.datetime_tel))\n",
    "comp1_2_3_4 = (comp1.join(comp2_3_4, ((comp1['machineID'] == comp2_3_4['machineID']) \n",
    "                                  & (comp1['datetime_tel'] == comp2_3_4['datetime_tel'])), \"left\")\n",
    "                                 .drop(comp2_3_4.machineID).drop(comp2_3_4.datetime_tel))\n",
    "comp1_2_3_4_final = (comp1_2_3_4.groupBy(\"machineID\", \"datetime_tel\")\n",
    "                                .agg(F.max('sincelastcomp1').alias('sincelastcomp1'), \n",
    "                                     F.max('sincelastcomp2').alias('sincelastcomp2'), \n",
    "                                     F.max('sincelastcomp3').alias('sincelastcomp3'), \n",
    "                                     F.max('sincelastcomp4').alias('sincelastcomp4')))\n",
    "\n",
    "# fill in missing value\n",
    "maint_count1 = comp1_2_3_4_final.fillna(0)\n",
    "\n",
    "maint_count1.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample to every 3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------+--------------+--------------+--------------+--------------------+\n",
      "|machineID|        datetime_tel|sincelastcomp1|sincelastcomp2|sincelastcomp3|sincelastcomp4|        dt_truncated|\n",
      "+---------+--------------------+--------------+--------------+--------------+--------------+--------------------+\n",
      "|        1|2015-01-01 06:00:...|           109|           109|            49|           184|2015-01-01 06:00:...|\n",
      "|        1|2015-01-05 10:00:...|           113|           113|            53|           188|2015-01-05 09:00:...|\n",
      "|        1|2015-01-10 18:00:...|           118|           118|            58|           193|2015-01-10 18:00:...|\n",
      "|        1|2015-01-10 20:00:...|           118|           118|            58|           193|2015-01-10 21:00:...|\n",
      "|        1|2015-01-11 13:00:...|           119|           119|            59|           194|2015-01-11 12:00:...|\n",
      "+---------+--------------------+--------------+--------------+--------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('machineID', 'int'),\n",
       " ('datetime_tel', 'timestamp'),\n",
       " ('sincelastcomp1', 'int'),\n",
       " ('sincelastcomp2', 'int'),\n",
       " ('sincelastcomp3', 'int'),\n",
       " ('sincelastcomp4', 'int'),\n",
       " ('dt_truncated', 'timestamp')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maint_count1 maintenance \n",
    "dt_truncated = ((round(unix_timestamp(col(\"datetime_tel\")) / time_val) * time_val)\n",
    "    .cast(\"timestamp\"))\n",
    "\n",
    "maint_resampled = maint_count1.withColumn(\"dt_truncated\", dt_truncated)\n",
    "maint_resampled1 = (maint_resampled.groupBy(\"machineID\",\"dt_truncated\")\n",
    "                                  .agg(F.mean('sincelastcomp1').alias('comp1sum'), \n",
    "                                       F.mean('sincelastcomp2').alias('comp2sum'), \n",
    "                                       F.mean('sincelastcomp3').alias('comp3sum'), \n",
    "                                       F.mean('sincelastcomp4').alias('comp4sum')))\n",
    "maint_resampled1.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine features - need to do one hot encoding for variable model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+-------------+\n",
      "|machineID| model|age|model_encoded|\n",
      "+---------+------+---+-------------+\n",
      "|        1|model2| 18|(3,[2],[1.0])|\n",
      "|        2|model4|  7|(3,[1],[1.0])|\n",
      "|        3|model3|  8|(3,[0],[1.0])|\n",
      "|        4|model3|  7|(3,[0],[1.0])|\n",
      "|        5|model2|  2|(3,[2],[1.0])|\n",
      "+---------+------+---+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding of the variable model\n",
    "catVarNames = ['model']  \n",
    "    \n",
    "sIndexers = [StringIndexer(inputCol=x, outputCol=x + '_indexed') for x in catVarNames]\n",
    "\n",
    "machines_cat = Pipeline(stages=sIndexers).fit(machines).transform(machines)\n",
    "\n",
    "# one-hot encode\n",
    "ohEncoders = [OneHotEncoder(inputCol=x + '_indexed', outputCol=x + '_encoded')\n",
    "              for x in catVarNames]\n",
    "ohPipelineModel = Pipeline(stages=ohEncoders).fit(machines_cat)\n",
    "machines_cat = ohPipelineModel.transform(machines_cat)\n",
    "\n",
    "drop_list = [col_n for col_n in machines_cat.columns if 'indexed' in col_n]\n",
    "\n",
    "machines_edit = machines_cat.select([column for column in machines_cat.columns if column not in drop_list])\n",
    "\n",
    "machines_edit.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating final feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+--------+--------+--------+--------+------+---+-------------+\n",
      "|machineID|dt_truncated         |volt_rollingmean_3|rotate_rollingmean_3|pressure_rollingmean_3|vibration_rollingmean_3|volt_rollingmean_24|rotate_rollingmean_24|pressure_rollingmean_24|vibration_rollingmean_24|volt_rollingstd_3 |rotate_rollingstd_3|pressure_rollingstd_3|vibration_rollingstd_3|volt_rollingstd_24|rotate_rollingstd_24|pressure_rollingstd_24|vibration_rollingstd_24|error1sum_rollingmean_24|error2sum_rollingmean_24|error3sum_rollingmean_24|error4sum_rollingmean_24|error5sum_rollingmean_24|comp1sum|comp2sum|comp3sum|comp4sum|model |age|model_encoded|\n",
      "+---------+---------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+--------+--------+--------+--------+------+---+-------------+\n",
      "|1        |2015-01-01 06:00:00.0|157.57049930335302|531.9910662461275   |104.6551338198705     |47.581986486218256     |157.57049930335302 |531.9910662461275    |104.6551338198705      |47.581986486218256      |7.991014579473104 |1.6652204764740581 |4.054491678023019    |2.859578012021218     |7.991014579473104 |1.6652204764740581  |4.054491678023019     |2.859578012021218      |0.0                     |0.0                     |0.0                     |0.0                     |0.0                     |109.0   |109.0   |49.0    |184.0   |model2|18 |(3,[2],[1.0])|\n",
      "|1        |2015-01-10 18:00:00.0|171.1276458072041 |475.01905654747424  |99.8149810921855      |42.13214174504382      |173.87941496436815 |459.5098021280746    |100.9136578469683      |41.06623144344391       |9.400726871368077 |42.014496475632775 |8.766643438461378    |5.368931756770667     |10.61409966839676 |54.64227648121238   |9.7604776846117       |5.412159334746818      |0.0                     |0.0                     |0.0                     |0.0                     |0.0                     |118.0   |118.0   |58.0    |193.0   |model2|18 |(3,[2],[1.0])|\n",
      "|1        |2015-01-13 21:00:00.0|183.51200471116002|414.7014468952589   |105.77143633090823    |37.93757382712593      |173.78536717380052 |442.09168579783324   |103.11615784391375     |39.42346411238715       |14.770945076258016|30.87434691606195  |7.7649814110512585   |7.753230618702326     |14.653042857476299|58.24802393934973   |10.313338684321076    |5.997009954995438      |0.0                     |0.0                     |0.0                     |0.0                     |0.0                     |121.0   |121.0   |61.0    |196.0   |model2|18 |(3,[2],[1.0])|\n",
      "|1        |2015-01-14 21:00:00.0|177.34692793630475|422.38265211812535  |102.2104586869488     |36.82785079481529      |166.6498926186019  |435.5865678554589    |101.90950154383667     |38.71746489569605       |6.963761158194722 |55.1895744851395   |7.003962452799105    |3.5611118332930136    |14.395789649351883|49.35504211554048   |7.995001614839825     |5.636093715315423      |0.0                     |0.0                     |0.0                     |0.0                     |0.0                     |122.0   |122.0   |62.0    |197.0   |model2|18 |(3,[2],[1.0])|\n",
      "|1        |2015-03-03 03:00:00.0|174.7224554311209 |413.0051134866578   |95.10431359751358     |42.75150527422306      |176.06598512315864 |452.0001041423295    |99.06924375724724      |40.45710336450855       |11.919261247404618|68.91375842322772  |9.361709879175843    |6.705147664897911     |20.25093179134953 |63.578084624953526  |9.693465153346265     |5.328810163002381      |0.0                     |0.0                     |0.0                     |0.0                     |0.0                     |170.0   |170.0   |110.0   |245.0   |model2|18 |(3,[2],[1.0])|\n",
      "+---------+---------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+--------+--------+--------+--------+------+---+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join error with components\n",
    "#err_mean_resampled1.show(3)\n",
    "#maint_resampled1.show(3)\n",
    "\n",
    "error_maint = (err_mean_resampled1.join(maint_resampled1, \n",
    "                                ((err_mean_resampled1['machineID'] == maint_resampled1['machineID']) \n",
    "                                  & (err_mean_resampled1['dt_truncated'] == maint_resampled1['dt_truncated'])), \"left\")\n",
    "                                  .drop(maint_resampled1.machineID).drop(maint_resampled1.dt_truncated))\n",
    "#error_maint.show(10, False)\n",
    "#error_maint.count(), len(error_maint.columns)\n",
    "\n",
    "# now join with machines\n",
    "#machines_edit.show(1)\n",
    "\n",
    "err_maint_mach = (error_maint.join(machines_edit, ((error_maint['machineID'] == machines_edit['machineID'])), \"left\")\n",
    "                             .drop(machines_edit.machineID))\n",
    "err_maint_mach_select = (err_maint_mach.select([c for c in err_maint_mach.columns if c not in \n",
    "                                               {'error1sum', 'error2sum', 'error3sum', 'error4sum', 'error5sum'}]))\n",
    "#err_maint_mach_select.show(10, False)\n",
    "#err_maint_mach_select.count(), len(err_maint_mach_select.columns)\n",
    "\n",
    "telemetry_all = (tel_mean_resampled1.join(tel_sd_resampled1, \n",
    "                             ((tel_mean_resampled1['machineID'] == tel_sd_resampled1['machineID']) \n",
    "                              & (tel_mean_resampled1['dt_truncated'] == tel_sd_resampled1['dt_truncated'])), \"left\")\n",
    "                              .drop(tel_sd_resampled1.machineID).drop(tel_sd_resampled1.dt_truncated))\n",
    "#telemetry_all.show(10, False)\n",
    "#telemetry_all.count(), len(telemetry_all.columns)\n",
    "\n",
    "# join telemetry_all with err_maint_mach_select to create final feature matrix\n",
    "final_feat = (telemetry_all.join(err_maint_mach_select, \n",
    "                                ((telemetry_all['machineID'] == err_maint_mach_select['machineID']) \n",
    "                                  & (telemetry_all['dt_truncated'] == err_maint_mach_select['dt_truncated'])), \"left\")\n",
    "                                 .drop(err_maint_mach_select.machineID).drop(err_maint_mach_select.dt_truncated))\n",
    "final_feat.show(5, False)\n",
    "#final_feat.count(), len(final_feat.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            datetime|machineID|failure|\n",
      "+--------------------+---------+-------+\n",
      "|2015-02-04 06:00:...|        1|  comp3|\n",
      "|2015-03-21 06:00:...|        1|  comp1|\n",
      "|2015-04-05 06:00:...|        1|  comp4|\n",
      "|2015-05-05 06:00:...|        1|  comp3|\n",
      "|2015-05-20 06:00:...|        1|  comp2|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6726, 3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check failure sample data\n",
    "failures.show(5)\n",
    "\n",
    "# check the dimensions of the data\n",
    "failures.count(), len(failures.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6368, 3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see if there are duplicate rows based on machine, datetime\n",
    "failures1 = failures.dropDuplicates(['machineID', 'datetime'])\n",
    "\n",
    "# check the dimensions of the data\n",
    "failures1.count(), len(failures1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+--------+--------+--------+--------+------+---+-------------+-------+\n",
      "|machineID|dt_truncated         |volt_rollingmean_3|rotate_rollingmean_3|pressure_rollingmean_3|vibration_rollingmean_3|volt_rollingmean_24|rotate_rollingmean_24|pressure_rollingmean_24|vibration_rollingmean_24|volt_rollingstd_3 |rotate_rollingstd_3|pressure_rollingstd_3|vibration_rollingstd_3|volt_rollingstd_24|rotate_rollingstd_24|pressure_rollingstd_24|vibration_rollingstd_24|error1sum_rollingmean_24|error2sum_rollingmean_24|error3sum_rollingmean_24|error4sum_rollingmean_24|error5sum_rollingmean_24|comp1sum|comp2sum|comp3sum|comp4sum|model |age|model_encoded|failure|\n",
      "+---------+---------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+--------+--------+--------+--------+------+---+-------------+-------+\n",
      "|1        |2015-01-01 06:00:00.0|157.57049930335302|531.9910662461275   |104.6551338198705     |47.581986486218256     |157.57049930335302 |531.9910662461275    |104.6551338198705      |47.581986486218256      |7.991014579473104 |1.6652204764740581 |4.054491678023019    |2.859578012021218     |7.991014579473104 |1.6652204764740581  |4.054491678023019     |2.859578012021218      |0.0                     |0.0                     |0.0                     |0.0                     |0.0                     |109.0   |109.0   |49.0    |184.0   |model2|18 |(3,[2],[1.0])|null   |\n",
      "|1        |2015-01-10 18:00:00.0|171.1276458072041 |475.01905654747424  |99.8149810921855      |42.13214174504382      |173.87941496436815 |459.5098021280746    |100.9136578469683      |41.06623144344391       |9.400726871368077 |42.014496475632775 |8.766643438461378    |5.368931756770667     |10.61409966839676 |54.64227648121238   |9.7604776846117       |5.412159334746818      |0.0                     |0.0                     |0.0                     |0.0                     |0.0                     |118.0   |118.0   |58.0    |193.0   |model2|18 |(3,[2],[1.0])|null   |\n",
      "|1        |2015-01-13 21:00:00.0|183.51200471116002|414.7014468952589   |105.77143633090823    |37.93757382712593      |173.78536717380052 |442.09168579783324   |103.11615784391375     |39.42346411238715       |14.770945076258016|30.87434691606195  |7.7649814110512585   |7.753230618702326     |14.653042857476299|58.24802393934973   |10.313338684321076    |5.997009954995438      |0.0                     |0.0                     |0.0                     |0.0                     |0.0                     |121.0   |121.0   |61.0    |196.0   |model2|18 |(3,[2],[1.0])|null   |\n",
      "|1        |2015-01-14 21:00:00.0|177.34692793630475|422.38265211812535  |102.2104586869488     |36.82785079481529      |166.6498926186019  |435.5865678554589    |101.90950154383667     |38.71746489569605       |6.963761158194722 |55.1895744851395   |7.003962452799105    |3.5611118332930136    |14.395789649351883|49.35504211554048   |7.995001614839825     |5.636093715315423      |0.0                     |0.0                     |0.0                     |0.0                     |0.0                     |122.0   |122.0   |62.0    |197.0   |model2|18 |(3,[2],[1.0])|null   |\n",
      "|1        |2015-03-03 03:00:00.0|174.7224554311209 |413.0051134866578   |95.10431359751358     |42.75150527422306      |176.06598512315864 |452.0001041423295    |99.06924375724724      |40.45710336450855       |11.919261247404618|68.91375842322772  |9.361709879175843    |6.705147664897911     |20.25093179134953 |63.578084624953526  |9.693465153346265     |5.328810163002381      |0.0                     |0.0                     |0.0                     |0.0                     |0.0                     |170.0   |170.0   |110.0   |245.0   |model2|18 |(3,[2],[1.0])|null   |\n",
      "+---------+---------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+--------+--------+--------+--------+------+---+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# map the failure data to final feature matrix\n",
    "\n",
    "labeled_features = (final_feat.join(failures1, ((final_feat['machineID'] == failures1['machineID']) \n",
    "                                  & (final_feat['dt_truncated'] == failures1['datetime'])), \"left\")\n",
    "                                  .drop(failures1.machineID).drop(failures1.datetime))\n",
    "labeled_features.show(5, False)\n",
    "#labeled_features.count(), len(labeled_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recoding the column 'failure' to be numeric double for the pyspark classification models\n",
    "labeled_features1 = (labeled_features.withColumn('failure', F.when(col('failure') == \"comp1\", 1.0)\n",
    "                                     .otherwise(col('failure')))\n",
    "                                     .withColumn('failure', F.when(col('failure') == \"comp2\", 2.0)\n",
    "                                     .otherwise(col('failure')))\n",
    "                                     .withColumn('failure', F.when(col('failure') == \"comp3\", 3.0)\n",
    "                                     .otherwise(col('failure')))\n",
    "                                     .withColumn('failure', F.when(col('failure') == \"comp4\", 4.0)\n",
    "                                     .otherwise(col('failure'))))\n",
    "\n",
    "labeled_features2 = labeled_features1.withColumn(\"failure1\", labeled_features1[\"failure\"].cast(DoubleType()))\n",
    "\n",
    "#labeled_features2.groupBy('failure').count().show()\n",
    "#labeled_features2.groupBy('failure1').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machineID', 'int'),\n",
       " ('dt_truncated', 'timestamp'),\n",
       " ('volt_rollingmean_3', 'double'),\n",
       " ('rotate_rollingmean_3', 'double'),\n",
       " ('pressure_rollingmean_3', 'double'),\n",
       " ('vibration_rollingmean_3', 'double'),\n",
       " ('volt_rollingmean_24', 'double'),\n",
       " ('rotate_rollingmean_24', 'double'),\n",
       " ('pressure_rollingmean_24', 'double'),\n",
       " ('vibration_rollingmean_24', 'double'),\n",
       " ('volt_rollingstd_3', 'double'),\n",
       " ('rotate_rollingstd_3', 'double'),\n",
       " ('pressure_rollingstd_3', 'double'),\n",
       " ('vibration_rollingstd_3', 'double'),\n",
       " ('volt_rollingstd_24', 'double'),\n",
       " ('rotate_rollingstd_24', 'double'),\n",
       " ('pressure_rollingstd_24', 'double'),\n",
       " ('vibration_rollingstd_24', 'double'),\n",
       " ('error1sum_rollingmean_24', 'double'),\n",
       " ('error2sum_rollingmean_24', 'double'),\n",
       " ('error3sum_rollingmean_24', 'double'),\n",
       " ('error4sum_rollingmean_24', 'double'),\n",
       " ('error5sum_rollingmean_24', 'double'),\n",
       " ('comp1sum', 'double'),\n",
       " ('comp2sum', 'double'),\n",
       " ('comp3sum', 'double'),\n",
       " ('comp4sum', 'double'),\n",
       " ('model', 'string'),\n",
       " ('age', 'int'),\n",
       " ('model_encoded', 'vector'),\n",
       " ('failure', 'string'),\n",
       " ('failure1', 'double')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data schema\n",
    "labeled_features2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machineID', 'int'),\n",
       " ('dt_truncated', 'timestamp'),\n",
       " ('volt_rollingmean_3', 'double'),\n",
       " ('rotate_rollingmean_3', 'double'),\n",
       " ('pressure_rollingmean_3', 'double'),\n",
       " ('vibration_rollingmean_3', 'double'),\n",
       " ('volt_rollingmean_24', 'double'),\n",
       " ('rotate_rollingmean_24', 'double'),\n",
       " ('pressure_rollingmean_24', 'double'),\n",
       " ('vibration_rollingmean_24', 'double'),\n",
       " ('volt_rollingstd_3', 'double'),\n",
       " ('rotate_rollingstd_3', 'double'),\n",
       " ('pressure_rollingstd_3', 'double'),\n",
       " ('vibration_rollingstd_3', 'double'),\n",
       " ('volt_rollingstd_24', 'double'),\n",
       " ('rotate_rollingstd_24', 'double'),\n",
       " ('pressure_rollingstd_24', 'double'),\n",
       " ('vibration_rollingstd_24', 'double'),\n",
       " ('error1sum_rollingmean_24', 'double'),\n",
       " ('error2sum_rollingmean_24', 'double'),\n",
       " ('error3sum_rollingmean_24', 'double'),\n",
       " ('error4sum_rollingmean_24', 'double'),\n",
       " ('error5sum_rollingmean_24', 'double'),\n",
       " ('comp1sum', 'double'),\n",
       " ('comp2sum', 'double'),\n",
       " ('comp3sum', 'double'),\n",
       " ('comp4sum', 'double'),\n",
       " ('model', 'string'),\n",
       " ('age', 'int'),\n",
       " ('model_encoded', 'vector'),\n",
       " ('failure1', 'double')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_features3 = labeled_features2.drop('failure').fillna(0)\n",
    "labeled_features3.dtypes\n",
    "#labeled_features3.groupBy('failure1').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+-----------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+--------+--------+--------+--------+------+---+-------------+--------+\n",
      "|machineID|        dt_truncated|volt_rollingmean_3|rotate_rollingmean_3|pressure_rollingmean_3|vibration_rollingmean_3|volt_rollingmean_24|rotate_rollingmean_24|pressure_rollingmean_24|vibration_rollingmean_24|volt_rollingstd_3|rotate_rollingstd_3|pressure_rollingstd_3|vibration_rollingstd_3|volt_rollingstd_24|rotate_rollingstd_24|pressure_rollingstd_24|vibration_rollingstd_24|error1sum_rollingmean_24|error2sum_rollingmean_24|error3sum_rollingmean_24|error4sum_rollingmean_24|error5sum_rollingmean_24|comp1sum|comp2sum|comp3sum|comp4sum| model|age|model_encoded|failure1|\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+-----------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+--------+--------+--------+--------+------+---+-------------+--------+\n",
      "|        1|2015-01-01 06:00:...|157.57049930335302|   531.9910662461275|     104.6551338198705|     47.581986486218256| 157.57049930335302|    531.9910662461275|      104.6551338198705|      47.581986486218256|7.991014579473104| 1.6652204764740581|    4.054491678023019|     2.859578012021218| 7.991014579473104|  1.6652204764740581|     4.054491678023019|      2.859578012021218|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|   109.0|   109.0|    49.0|   184.0|model2| 18|(3,[2],[1.0])|     0.0|\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+-----------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+--------+--------+--------+--------+------+---+-------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the code for backfill with all machine data\n",
    "label_bfill1 = labeled_features3\n",
    "label_bfill1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lag values to manually backfill label (bfill =7)\n",
    "my_window = Window.partitionBy('machineID').orderBy(label_bfill1.dt_truncated.desc())\n",
    "\n",
    "label_bfill1 = label_bfill1.withColumn(\"prev_value1\", F.lag(label_bfill1.failure1).over(my_window)).fillna(0)\n",
    "label_bfill1 = label_bfill1.withColumn(\"prev_value2\", F.lag(label_bfill1.prev_value1).over(my_window)).fillna(0) \n",
    "label_bfill1 = label_bfill1.withColumn(\"prev_value3\", F.lag(label_bfill1.prev_value2).over(my_window)).fillna(0) \n",
    "label_bfill1 = label_bfill1.withColumn(\"prev_value4\", F.lag(label_bfill1.prev_value3).over(my_window)).fillna(0) \n",
    "label_bfill1 = label_bfill1.withColumn(\"prev_value5\", F.lag(label_bfill1.prev_value4).over(my_window)).fillna(0) \n",
    "label_bfill1 = label_bfill1.withColumn(\"prev_value6\", F.lag(label_bfill1.prev_value5).over(my_window)).fillna(0) \n",
    "label_bfill1 = label_bfill1.withColumn(\"prev_value7\", F.lag(label_bfill1.prev_value6).over(my_window)).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the label column \n",
    "label_bfill2 = (label_bfill1.withColumn('label', label_bfill1.failure1 + label_bfill1.prev_value1 \n",
    "                         + label_bfill1.prev_value2 + label_bfill1.prev_value3 + label_bfill1.prev_value4 \n",
    "                         + label_bfill1.prev_value5 + label_bfill1.prev_value6 + label_bfill1.prev_value7))\n",
    "label_bfill2 = label_bfill2.withColumn('label_e', F.when(col('label') > 4, 4.0).otherwise(col('label')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_bfill3 = (label_bfill2.drop(label_bfill2.prev_value1).drop(label_bfill2.prev_value2)\n",
    "              .drop(label_bfill2.prev_value3).drop(label_bfill2.prev_value4)\n",
    "              .drop(label_bfill2.prev_value5).drop(label_bfill2.prev_value6)\n",
    "              .drop(label_bfill2.prev_value7).drop(label_bfill2.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+-----------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+--------+--------+--------+--------+------+---+-------------+--------+-------+\n",
      "|machineID|        dt_truncated|volt_rollingmean_3|rotate_rollingmean_3|pressure_rollingmean_3|vibration_rollingmean_3|volt_rollingmean_24|rotate_rollingmean_24|pressure_rollingmean_24|vibration_rollingmean_24|volt_rollingstd_3|rotate_rollingstd_3|pressure_rollingstd_3|vibration_rollingstd_3|volt_rollingstd_24|rotate_rollingstd_24|pressure_rollingstd_24|vibration_rollingstd_24|error1sum_rollingmean_24|error2sum_rollingmean_24|error3sum_rollingmean_24|error4sum_rollingmean_24|error5sum_rollingmean_24|comp1sum|comp2sum|comp3sum|comp4sum| model|age|model_encoded|failure1|label_e|\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+-----------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+--------+--------+--------+--------+------+---+-------------+--------+-------+\n",
      "|      148|2016-01-01 06:00:...| 167.6093732259465|  468.37635987579284|    103.88896088440748|      39.94322894012483| 173.51403833166742|     441.926447636747|     102.99625303698696|      41.143505398991664| 22.5318014229406|  42.37962933847919|    8.878567461506137|    3.5246531682381956|17.731479371681786|   51.60934598661173|     9.169245334825082|     5.9766629305814964|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|   474.0|   429.0|   474.0|   444.0|model3|  9|(3,[0],[1.0])|     0.0|    0.0|\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+-----------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+--------+--------+--------+--------+------+---+-------------+--------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_bfill3.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-1131ff50e057>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# upload the entire folder into blob storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'featureengineering_files.parquet/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmy_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_blob_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTAINER_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n"
     ]
    }
   ],
   "source": [
    "# write the final result as parquet file in blob location \n",
    "# https://github.com/Azure/ViennaDocs/blob/master/Documentation/UsingBlobForStorage.md\n",
    "CONTAINER_NAME = \"featureengineering\"\n",
    "\n",
    "# Create a new container if necessary, otherwise you can use an existing container.\n",
    "# This command creates the container if it does not already exist. Else it does nothing.\n",
    "my_service.create_container(CONTAINER_NAME, \n",
    "                            fail_on_exist=False, \n",
    "                            public_access=PublicAccess.Container)\n",
    "\n",
    "# you decide to partition the dataframe into three files and save them in the current folder.\n",
    "# if you wish to visualize them in the run history Output Files, specify the path \n",
    "# as './outputs/multiple_files.parquet'.\n",
    "#label_bfill3.coalesce(3).write.mode('overwrite').parquet('multiple_files.parquet')\n",
    "label_bfill3.write.mode('overwrite').parquet('featureengineering_files.parquet')\n",
    "\n",
    "# unlike the single file case, for multiple files we need to first delete results from the \n",
    "# previous run before uploading.\n",
    "for blob in my_service.list_blobs(CONTAINER_NAME):\n",
    "    if 'featureengineering_files.parquet' in blob.name:\n",
    "        my_service.delete_blob(CONTAINER_NAME, blob.name)\n",
    "\n",
    "# upload the entire folder into blob storage\n",
    "for name in glob.iglob('featureengineering_files.parquet/*'):\n",
    "    print(os.path.abspath(name))\n",
    "    my_service.create_blob_from_path(CONTAINER_NAME, name, name)\n",
    "\n",
    "print(\"Feature engineering final dataset files saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hack2 docker",
   "language": "python",
   "name": "hack2_docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

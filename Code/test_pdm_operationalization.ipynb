{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4a: Model operationalization & Deployment\n",
    "\n",
    "The best model is saved as a .model file along with the relevant scheme for deployment. The functions are first tested locally before operationalizing the model using Azure Machine Learning Model Management environment for use in production in realtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup our environment by importing required libraries\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "\n",
    "import glob\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess\n",
    "\n",
    "# For creating pipelines and model\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Setup the pyspark environment\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE creating a local directory!\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+-----------------+-----------------+-----------------+-----------------+------+---+-------------+--------+-------+\n",
      "|machineID|        dt_truncated|volt_rollingmean_3|rotate_rollingmean_3|pressure_rollingmean_3|vibration_rollingmean_3|volt_rollingmean_24|rotate_rollingmean_24|pressure_rollingmean_24|vibration_rollingmean_24| volt_rollingstd_3|rotate_rollingstd_3|pressure_rollingstd_3|vibration_rollingstd_3|volt_rollingstd_24|rotate_rollingstd_24|pressure_rollingstd_24|vibration_rollingstd_24|error1sum_rollingmean_24|error2sum_rollingmean_24|error3sum_rollingmean_24|error4sum_rollingmean_24|error5sum_rollingmean_24|         comp1sum|         comp2sum|         comp3sum|         comp4sum| model|age|model_encoded|failure1|label_e|\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+-----------------+-----------------+-----------------+-----------------+------+---+-------------+--------+-------+\n",
      "|      114|2016-01-01 06:00:...| 163.3757329023745|  333.14948458556535|    100.18395169796328|     44.095881263819514| 164.11472399129667|     277.191815231867|      97.62891107072754|       50.88535051605059|21.004956521854176|  67.52872593782637|   12.936152686125933|     4.613597609179632|15.537773806210083|   67.65198854409817|     10.52827463297973|      6.941294875549091|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|            489.0|            549.0|            549.0|            564.0|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "|      114|2016-01-01 03:00:...| 168.0560723838507|  279.96957534682707|     97.63372820912399|      48.24221679536314| 164.03977789009616|    265.4140301347857|       98.2692713270434|      51.574366655467436|17.820643824484396| 49.637429003930606|    9.789304756066265|     7.963344413644408| 15.48856269342518|   67.26398825199145|    10.343146088525474|      6.738109627670492|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|            489.0|            549.0|            549.0|            564.0|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "|      114|2016-01-01 00:00:...|161.59623278663096|   277.7171441565807|     95.71420759202182|     51.508182534644625| 164.56778047540993|   265.99342004784285|       99.6572121485503|       50.46248490865013|20.582267478545578|  80.97739710207937|    8.128630498837182|     10.04603679663133|15.551165354528889|   65.23789560931164|    11.490148490289625|     7.7457818689251345|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|488.6666666666667|548.6666666666666|548.6666666666666|563.6666666666666|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "|      114|2015-12-31 21:00:...| 170.0141386667267|   292.4471663647502|      86.0653955365305|     50.314535427527936| 165.41723263438084|   278.12231772796457|      98.87594592461198|      49.100686634977116|13.812544186838844| 56.224636185244115|    9.605933766887967|     6.294693994764171|15.402539242532207|   78.42674708913566|    12.387032843966068|      7.822982220380671|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|            488.0|            548.0|            548.0|            563.0|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "|      114|2015-12-31 18:00:...|170.42359844411342|   334.3696511783739|     99.07688677874042|     48.853143551499244|  166.3251929489545|    286.4962812101407|     101.27560815245654|      47.541499460975025|  8.73668143235762|  44.16549629157097|    5.827054526494244|     6.890586104508928|15.088084779349089|   81.86613174940224|    11.384795095243218|      8.422065747978259|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|            488.0|            548.0|            548.0|            563.0|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+-----------------+-----------------+-----------------+-----------------+------+---+-------------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Feature engineering final dataset files loaded!\n",
      "CPU times: user 12.6 s, sys: 4.94 s, total: 17.5 s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the previous created final dataset into the workspace\n",
    "from azure.storage.blob import BlockBlobService\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# define parameters \n",
    "ACCOUNT_NAME = \"pdmvienna\"\n",
    "ACCOUNT_KEY = \"PDuXK61GpmMVWMrWdvr29THbPdlOXa61fN5RfgQV/jBO8berC1zLzZ678Nxrx+D3CRp4+ZvSff9al+lrUh8qUQ==\"\n",
    "CONTAINER_NAME = \"featureengineering\"\n",
    "\n",
    "# define your blob service     \n",
    "my_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# create a local path where to store the results later.\n",
    "LOCAL_DIRECT = 'model_operationalize.parquet'\n",
    "if not os.path.exists(LOCAL_DIRECT):\n",
    "    os.makedirs(LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# define your blob service     \n",
    "my_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in my_service.list_blobs(CONTAINER_NAME):\n",
    "    if 'featureengineering_files.parquet' in blob.name:\n",
    "        local_file = os.path.join(LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        my_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "data = spark.read.parquet('model_operationalize.parquet')\n",
    "#data.persist()\n",
    "data.show(5)\n",
    "print('Feature engineering final dataset files loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the features, labels for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of input columns for downstream modeling - note model variable was removed as string was not supported\n",
    "input_features = [\n",
    "'volt_rollingmean_3',\n",
    "'rotate_rollingmean_3',\n",
    "'pressure_rollingmean_3',\n",
    "'vibration_rollingmean_3',\n",
    "'volt_rollingmean_24',\n",
    "'rotate_rollingmean_24',\n",
    "'pressure_rollingmean_24',\n",
    "'vibration_rollingmean_24',\n",
    "'volt_rollingstd_3',\n",
    "'rotate_rollingstd_3',\n",
    "'pressure_rollingstd_3',\n",
    "'vibration_rollingstd_3',\n",
    "'volt_rollingstd_24',\n",
    "'rotate_rollingstd_24',\n",
    "'pressure_rollingstd_24',\n",
    "'vibration_rollingstd_24',\n",
    "'error1sum_rollingmean_24',\n",
    "'error2sum_rollingmean_24',\n",
    "'error3sum_rollingmean_24',\n",
    "'error4sum_rollingmean_24',\n",
    "'error5sum_rollingmean_24',\n",
    "'comp1sum',\n",
    "'comp2sum',\n",
    "'comp3sum',\n",
    "'comp4sum',\n",
    "'age'  \n",
    "]\n",
    "\n",
    "label_var = ['label_e']\n",
    "key_cols =['machineID','dt_truncated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble features\n",
    "va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "data = va.transform(data).select('machineID','dt_truncated','label_e','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set maxCategories so features with > 10 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \n",
    "                               outputCol=\"indexedFeatures\", \n",
    "                               maxCategories=10).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit on whole dataset to include all labels in index\n",
    "labelIndexer = StringIndexer(inputCol=\"label_e\", outputCol=\"indexedLabel\").fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2174000\n",
      "747000\n"
     ]
    }
   ],
   "source": [
    "# split the data into train/test based on date\n",
    "training = data.filter(data.dt_truncated > \"2015-01-01\").filter(data.dt_truncated < \"2015-09-30\")\n",
    "testing = data.filter(data.dt_truncated > \"2015-09-30\")\n",
    "\n",
    "print(training.count())\n",
    "print(testing.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# chain indexers and forest in a Pipeline\n",
    "pipeline_rf = Pipeline(stages=[labelIndexer, featureIndexer, rf])\n",
    "\n",
    "# train model.  This also runs the indexers.\n",
    "model_rf = pipeline_rf.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions.\n",
    "predictions_rf = model_rf.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = predictions_rf.select(\"indexedLabel\", \"prediction\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.994112\n"
     ]
    }
   ],
   "source": [
    "# select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\")\n",
    "print(\"Accuracy = %g\" % evaluator.evaluate(predictions_rf, {evaluator.metricName: \"accuracy\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your model and schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a model that performs well, you can package it into a scoring service. To prepare for this, save your model and dataset schema locally first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_rf.write().overwrite().save(\"/azureml-share/pdmrfull.model\")\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdmrfull.model\r\n"
     ]
    }
   ],
   "source": [
    "# check to see if the model was saved in the shared location\n",
    "!ls /azureml-share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_operationalize docker",
   "language": "python",
   "name": "test_operationalize_docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
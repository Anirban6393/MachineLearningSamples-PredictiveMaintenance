{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Model operationalization & Deployment\n",
    "\n",
    "In this script, a model is saved as a .model file along with the relevant scheme for deployment. The functions are first tested locally before operationalizing the model using Azure Machine Learning Model Management environment for use in production in realtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## setup our environment by importing required libraries\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# setup the pyspark environment\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the features, labels for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define list of input columns for downstream modeling \n",
    "input_features = [\n",
    "'volt_rollingmean_3',\n",
    "'rotate_rollingmean_3',\n",
    "'pressure_rollingmean_3',\n",
    "'vibration_rollingmean_3',\n",
    "'volt_rollingmean_24',\n",
    "'rotate_rollingmean_24',\n",
    "'pressure_rollingmean_24',\n",
    "'vibration_rollingmean_24',\n",
    "'volt_rollingstd_3',\n",
    "'rotate_rollingstd_3',\n",
    "'pressure_rollingstd_3',\n",
    "'vibration_rollingstd_3',\n",
    "'volt_rollingstd_24',\n",
    "'rotate_rollingstd_24',\n",
    "'pressure_rollingstd_24',\n",
    "'vibration_rollingstd_24',\n",
    "'error1sum_rollingmean_24',\n",
    "'error2sum_rollingmean_24',\n",
    "'error3sum_rollingmean_24',\n",
    "'error4sum_rollingmean_24',\n",
    "'error5sum_rollingmean_24',\n",
    "'comp1sum',\n",
    "'comp2sum',\n",
    "'comp3sum',\n",
    "'comp4sum',\n",
    "'age'  \n",
    "]\n",
    "\n",
    "label_var = ['label_e']\n",
    "key_cols =['machineID','dt_truncated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authoring Realtime Web Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we show the user how to author a realtime web service that scores the model you saved above. First, check to ensure that the latest version of the azure-ml-api-sdk is available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define init and run functions\n",
    "Start by defining the init() and run() functions as shown in the cell below. Then write them to the score.py file. This file will load the model, perform the prediction, and return the result.\n",
    "\n",
    "The init() function initializes your web service, loading in any data or models that you need to score your inputs. In the example below, we load in the trained model. This command is run when the Docker container containing your service initializes.\n",
    "The run() function defines what is executed on a scoring call. In our simple example, we simply load in the input as a data frame, and run our pipeline on the input, and return the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    # read in the model file\n",
    "    from pyspark.ml import PipelineModel\n",
    "    global pipeline\n",
    "    \n",
    "    pipeline = PipelineModel.load(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']+'pdmrfull.model')\n",
    "    \n",
    "def run(input_df):\n",
    "    import json\n",
    "    response = ''\n",
    "    try:\n",
    "        #Get prediction results for the dataframe\n",
    "        input_features = [\n",
    "            'volt_rollingmean_3',\n",
    "            'rotate_rollingmean_3',\n",
    "            'pressure_rollingmean_3',\n",
    "            'vibration_rollingmean_3',\n",
    "            'volt_rollingmean_24',\n",
    "            'rotate_rollingmean_24',\n",
    "            'pressure_rollingmean_24',\n",
    "            'vibration_rollingmean_24',\n",
    "            'volt_rollingstd_3',\n",
    "            'rotate_rollingstd_3',\n",
    "            'pressure_rollingstd_3',\n",
    "            'vibration_rollingstd_3',\n",
    "            'volt_rollingstd_24',\n",
    "            'rotate_rollingstd_24',\n",
    "            'pressure_rollingstd_24',\n",
    "            'vibration_rollingstd_24',\n",
    "            'error1sum_rollingmean_24',\n",
    "            'error2sum_rollingmean_24',\n",
    "            'error3sum_rollingmean_24',\n",
    "            'error4sum_rollingmean_24',\n",
    "            'error5sum_rollingmean_24',\n",
    "            'comp1sum',\n",
    "            'comp2sum',\n",
    "            'comp3sum',\n",
    "            'comp4sum',\n",
    "            'age',\n",
    "        ]\n",
    "        \n",
    "        va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "        data = va.transform(input_df).select('machineID','features')\n",
    "        score = pipeline.transform(data)\n",
    "        predictions = score.collect()\n",
    "\n",
    "        #Get each scored result\n",
    "        preds = [str(x['prediction']) for x in predictions]\n",
    "        response = \",\".join(preds)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {0}\",str(e))\n",
    "        return (str(e))\n",
    "    \n",
    "    # Return results\n",
    "    print(json.dumps(response))\n",
    "    return json.dumps(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create schema and schema file\n",
    "Create a schema for the input to the web service and generate the schema file. This will be used to create a Swagger file for your web service which can be used to discover its input and sample data when calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the input data frame\n",
    "inputs = {\"input_df\": SampleDefinition(DataTypes.SPARK, data.drop(\"dt_truncated\",\"failure1\",\"label_e\", \"model\",\"model_encoded\"))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': {'input_df': {'internal': {'fields': [{'metadata': {}, 'type': 'integer', 'name': 'machineID', 'nullable': True}, {'metadata': {'ml_attr': {'num_attrs': 26, 'attrs': {'numeric': [{'idx': 0, 'name': 'volt_rollingmean_3'}, {'idx': 1, 'name': 'rotate_rollingmean_3'}, {'idx': 2, 'name': 'pressure_rollingmean_3'}, {'idx': 3, 'name': 'vibration_rollingmean_3'}, {'idx': 4, 'name': 'volt_rollingmean_24'}, {'idx': 5, 'name': 'rotate_rollingmean_24'}, {'idx': 6, 'name': 'pressure_rollingmean_24'}, {'idx': 7, 'name': 'vibration_rollingmean_24'}, {'idx': 8, 'name': 'volt_rollingstd_3'}, {'idx': 9, 'name': 'rotate_rollingstd_3'}, {'idx': 10, 'name': 'pressure_rollingstd_3'}, {'idx': 11, 'name': 'vibration_rollingstd_3'}, {'idx': 12, 'name': 'volt_rollingstd_24'}, {'idx': 13, 'name': 'rotate_rollingstd_24'}, {'idx': 14, 'name': 'pressure_rollingstd_24'}, {'idx': 15, 'name': 'vibration_rollingstd_24'}, {'idx': 16, 'name': 'error1sum_rollingmean_24'}, {'idx': 17, 'name': 'error2sum_rollingmean_24'}, {'idx': 18, 'name': 'error3sum_rollingmean_24'}, {'idx': 19, 'name': 'error4sum_rollingmean_24'}, {'idx': 20, 'name': 'error5sum_rollingmean_24'}, {'idx': 21, 'name': 'comp1sum'}, {'idx': 22, 'name': 'comp2sum'}, {'idx': 23, 'name': 'comp3sum'}, {'idx': 24, 'name': 'comp4sum'}, {'idx': 25, 'name': 'age'}]}}}, 'type': {'pyClass': 'pyspark.ml.linalg.VectorUDT', 'sqlType': {'fields': [{'metadata': {}, 'type': 'byte', 'name': 'type', 'nullable': False}, {'metadata': {}, 'type': 'integer', 'name': 'size', 'nullable': True}, {'metadata': {}, 'type': {'containsNull': False, 'elementType': 'integer', 'type': 'array'}, 'name': 'indices', 'nullable': True}, {'metadata': {}, 'type': {'containsNull': False, 'elementType': 'double', 'type': 'array'}, 'name': 'values', 'nullable': True}], 'type': 'struct'}, 'type': 'udt', 'class': 'org.apache.spark.ml.linalg.VectorUDT'}, 'name': 'features', 'nullable': True}], 'type': 'struct'}, 'type': 2, 'swagger': {'items': {'type': 'object', 'properties': {'features': {'type': 'object'}, 'machineID': {'format': 'int32', 'type': 'integer'}}}, 'type': 'array', 'example': [{'features': '[163.375732902,333.149484586,100.183951698,44.0958812638,164.114723991,277.191815232,97.6289110707,50.8853505161,21.0049565219,67.5287259378,12.9361526861,4.61359760918,15.5377738062,67.6519885441,10.528274633,6.94129487555,0.0,0.0,0.0,0.0,0.0,489.0,549.0,549.0,564.0,18.0]', 'machineID': 114}, {'features': '[168.056072384,279.969575347,97.6337282091,48.2422167954,164.03977789,265.414030135,98.269271327,51.5743666555,17.8206438245,49.6374290039,9.78930475607,7.96334441364,15.4885626934,67.263988252,10.3431460885,6.73810962767,0.0,0.0,0.0,0.0,0.0,489.0,549.0,549.0,564.0,18.0]', 'machineID': 114}, {'features': '[161.596232787,277.717144157,95.714207592,51.5081825346,164.567780475,265.993420048,99.6572121486,50.4624849087,20.5822674785,80.9773971021,8.12863049884,10.0460367966,15.5511653545,65.2378956093,11.4901484903,7.74578186893,0.0,0.0,0.0,0.0,0.0,488.666666667,548.666666667,548.666666667,563.666666667,18.0]', 'machineID': 114}]}}}}\n"
     ]
    }
   ],
   "source": [
    "x = generate_schema(run_func=run, inputs=inputs, filepath='service_schema.json')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test init and run\n",
    "We can then test the init() and run() functions right here in the notebook, before we decide to actually publish a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[114,\n",
       "  163.375732902,\n",
       "  333.149484586,\n",
       "  100.183951698,\n",
       "  44.0958812638,\n",
       "  164.114723991,\n",
       "  277.191815232,\n",
       "  97.6289110707,\n",
       "  50.8853505161,\n",
       "  21.0049565219,\n",
       "  67.5287259378,\n",
       "  12.9361526861,\n",
       "  4.61359760918,\n",
       "  15.5377738062,\n",
       "  67.6519885441,\n",
       "  10.528274633,\n",
       "  6.94129487555,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  489.0,\n",
       "  549.0,\n",
       "  549.0,\n",
       "  564.0,\n",
       "  18.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the input data should be\n",
    "input_data = [[114, 163.375732902,333.149484586,100.183951698,44.0958812638,164.114723991,277.191815232,97.6289110707,50.8853505161,21.0049565219,67.5287259378,12.9361526861,4.61359760918,15.5377738062,67.6519885441,10.528274633,6.94129487555,0.0,0.0,0.0,0.0,0.0,489.0,549.0,549.0,564.0,18.0]]\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = (spark.createDataFrame(input_data, [\"machineID\", \"volt_rollingmean_3\", \"rotate_rollingmean_3\", \"pressure_rollingmean_3\", \"vibration_rollingmean_3\", \"volt_rollingmean_24\", \n",
    "            \"rotate_rollingmean_24\", \"pressure_rollingmean_24\", \"vibration_rollingmean_24\", \"volt_rollingstd_3\", \"rotate_rollingstd_3\",\n",
    "            \"pressure_rollingstd_3\", \"vibration_rollingstd_3\", \"volt_rollingstd_24\", \"rotate_rollingstd_24\", \"pressure_rollingstd_24\",\n",
    "            \"vibration_rollingstd_24\", \"error1sum_rollingmean_24\", \"error2sum_rollingmean_24\", \"error3sum_rollingmean_24\",\n",
    "            \"error4sum_rollingmean_24\", \"error5sum_rollingmean_24\", \"comp1sum\", \"comp2sum\", \"comp3sum\", \"comp4sum\",\n",
    "            \"age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test init() in local notebook\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0.0\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"0.0\"'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test run() in local notebook\n",
    "run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the schema file for deployment\n",
    "out = json.dumps(x)\n",
    "with open(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'] + 'service_schema.json', 'w') as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdmrfull.model\tservice_schema.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls $AZUREML_NATIVE_SHARE_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the user will need to navigate to the folder: \n",
    "```C:\\Users\\<username>\\.azureml\\share\\<team account>\\<Project Name> ```\n",
    "\n",
    "Copy the file service_schema.json to your projects folder for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use %%writefile command will save the *.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /azureml-share/pdmscore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /azureml-share/pdmscore.py\n",
    "# after testing the below init() and run() functions,\n",
    "# uncomment this cell to create the score.py after.\n",
    "\n",
    "# remove import from init() from function.\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import concat, col, udf, lag, date_add, explode, lit, unix_timestamp\n",
    "from pyspark.sql.functions import month, weekofyear, dayofmonth\n",
    "from pyspark.sql.functions import datediff, to_date, lit, unix_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.dataframe import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.feature import StandardScaler, PCA, RFormula\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema\n",
    "\n",
    "\n",
    "def init():\n",
    "    # read in the model file\n",
    "    from pyspark.ml import PipelineModel\n",
    "    global pipeline\n",
    "    pipeline = PipelineModel.load(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']+'pdmrfull.model')\n",
    "    \n",
    "def run(input_df):\n",
    "    import json\n",
    "    response = ''\n",
    "    \n",
    "    try:\n",
    "        #Get prediction results for the dataframe\n",
    "        input_features = [\n",
    "            'volt_rollingmean_3',\n",
    "            'rotate_rollingmean_3',\n",
    "            'pressure_rollingmean_3',\n",
    "            'vibration_rollingmean_3',\n",
    "            'volt_rollingmean_24',\n",
    "            'rotate_rollingmean_24',\n",
    "            'pressure_rollingmean_24',\n",
    "            'vibration_rollingmean_24',\n",
    "            'volt_rollingstd_3',\n",
    "            'rotate_rollingstd_3',\n",
    "            'pressure_rollingstd_3',\n",
    "            'vibration_rollingstd_3',\n",
    "            'volt_rollingstd_24',\n",
    "            'rotate_rollingstd_24',\n",
    "            'pressure_rollingstd_24',\n",
    "            'vibration_rollingstd_24',\n",
    "            'error1sum_rollingmean_24',\n",
    "            'error2sum_rollingmean_24',\n",
    "            'error3sum_rollingmean_24',\n",
    "            'error4sum_rollingmean_24',\n",
    "            'error5sum_rollingmean_24',\n",
    "            'comp1sum',\n",
    "            'comp2sum',\n",
    "            'comp3sum',\n",
    "            'comp4sum',\n",
    "            'age',\n",
    "        ]\n",
    "\n",
    "        va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "        data = va.transform(input_df).select('machineID','features')\n",
    "        score = pipeline.transform(data)\n",
    "        predictions = score.collect()\n",
    "\n",
    "        #Get each scored result\n",
    "        preds = [str(x['prediction']) for x in predictions]\n",
    "        response = \",\".join(preds)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {0}\",str(e))\n",
    "        return (str(e))\n",
    "    \n",
    "    # Return results\n",
    "    print(json.dumps(response))\n",
    "    return json.dumps(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init()\n",
    "    run(\"{\\\"input_df\\\":[{\\\"machineID\\\":114,\\\"volt_rollingmean_3\\\":163.375732902,\\\"rotate_rollingmean_3\\\":333.149484586,\\\"pressure_rollingmean_3\\\":100.183951698,\\\"vibration_rollingmean_3\\\":44.0958812638,\\\"volt_rollingmean_24\\\":164.114723991,\\\"rotate_rollingmean_24\\\":277.191815232,\\\"pressure_rollingmean_24\\\":97.6289110707,\\\"vibration_rollingmean_24\\\":50.8853505161,\\\"volt_rollingstd_3\\\":21.0049565219,\\\"rotate_rollingstd_3\\\":67.5287259378,\\\"pressure_rollingstd_3\\\":12.9361526861,\\\"vibration_rollingstd_3\\\":4.61359760918,\\\"volt_rollingstd_24\\\":15.5377738062,\\\"rotate_rollingstd_24\\\":67.6519885441,\\\"pressure_rollingstd_24\\\":10.528274633,\\\"vibration_rollingstd_24\\\":6.94129487555,\\\"error1sum_rollingmean_24\\\":0.0,\\\"error2sum_rollingmean_24\\\":0.0,\\\"error3sum_rollingmean_24\\\":0.0,\\\"error4sum_rollingmean_24\\\":0.0,\\\"error5sum_rollingmean_24\\\":0.0,\\\"comp1sum\\\":489.0,\\\"comp2sum\\\":549.0,\\\"comp3sum\\\":549.0,\\\"comp4sum\\\":564.0,\\\"age\\\":18.0}]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdmrfull.model\tpdmscore.py  service_schema.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls $AZUREML_NATIVE_SHARE_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the user will need to navigate to the folder: \n",
    "```C:\\Users\\<username>\\.azureml\\share\\<team account>\\<Project Name> ```\n",
    "\n",
    "Copy the file pdmscore.py to your projects folder for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the CLI to deploy and manage your web service \n",
    "\n",
    "## Pre-requisites \n",
    "\n",
    "Use the following commands to set up an environment and account to run the web service. For more info, see the Getting Started Guide and the CLI Command Reference. You can use -h flag at the end of the commands for command help.\n",
    "\n",
    "• Create the environment (you need to do this once per environment e.g. dev or prod)\n",
    "```\n",
    "az ml env setup -c -n <yourclustername> --location <e.g. eastus2>\n",
    "```\n",
    "\n",
    "• Create a Model Management account (one time setup)\n",
    "```\n",
    "az ml account modelmanagement create --location <e.g. eastus2> -n <your-new-acctname> -g <yourresourcegroupname> --sku-instances 1 --sku-name S1\n",
    "```\n",
    "\n",
    "• Set the Model Management account\n",
    "```\n",
    "az ml account modelmanagement set -n <youracctname> -g <yourresourcegroupname>\n",
    "```\n",
    "\n",
    "• Set the environment. The cluster name is the name used in step 1 above. The resource group name was the output of the same process and would be in the command window when the setup process is completed.\n",
    "```\n",
    "az ml env set -n <yourclustername> -g <yourresourcegroupname>\n",
    "```\n",
    "\n",
    "## Deploy your web service \n",
    "\n",
    "Switch to a bash shell, and run the following commands to deploy your service and run it.\n",
    "\n",
    "Enter the path where the notebook and other files are saved. Your actual path may be different from this example.\n",
    "```\n",
    "cd ~/notebooks/azureml/realtime/\n",
    "```\n",
    "\n",
    "This assumes that you saved your model locally.\n",
    "```\n",
    "az ml service create realtime -f pdmscore.py -r  spark-py -m pdmrfull.model -s service_schema.json -n pdmservice --cpu 0.1\n",
    "```\n",
    "\n",
    "This command will return the sample run command with sample data. You can get the Service Id from the output of the create command above.\n",
    "```\n",
    "az ml service show realtime -i <yourserviceid>\n",
    "```\n",
    "\n",
    "Call the web service to get a prediction\n",
    "```\n",
    "az ml service run realtime -i <yourserviceid> -d \"{\\\"input_df\\\": [{\\\"machineID\\\":114, \\\"vo\n",
    "lt_rollingmean_3\\\":163.375732902, \\\"rotate_rollingmean_3\\\":333.149484586, \\\"pressure_rollingmean_3\\\":100.183951698, \\\"vibration_rollingmean_3\\\":44.0958812638, \\\"volt_rollingme\n",
    "an_24\\\":164.114723991, \\\"rotate_rollingmean_24\\\":277.191815232, \\\"pressure_rollingmean_24\\\":97.6289110707, \\\"vibration_rollingmean_24\\\":50.8853505161, \\\"volt_rollingstd_3\\\":21\n",
    ".0049565219, \\\"rotate_rollingstd_3\\\":67.5287259378, \\\"pressure_rollingstd_3\\\":12.9361526861, \\\"vibration_rollingstd_3\\\":4.61359760918, \\\"volt_rollingstd_24\\\":15.5377738062, \\\"\n",
    "rotate_rollingstd_24\\\":67.6519885441, \\\"pressure_rollingstd_24\\\":10.528274633, \\\"vibration_rollingstd_24\\\":6.94129487555, \\\"error1sum_rollingmean_24\\\":0.0, \\\"error2sum_rolling\n",
    "mean_24\\\":0.0, \\\"error3sum_rollingmean_24\\\":0.0, \\\"error4sum_rollingmean_24\\\":0.0, \\\"error5sum_rollingmean_24\\\":0.0, \\\"comp1sum\\\":489.0, \\\"comp2sum\\\":549.0, \\\"comp3sum\\\":549.0\n",
    ", \\\"comp4sum\\\":564.0, \\\"age\\\":180}]}\"\n",
    "```\n",
    "\n",
    "Predicted output label is as follows:\n",
    "```\n",
    "\"0.0\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hack2 docker",
   "language": "python",
   "name": "hack2_docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
